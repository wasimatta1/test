{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11625954,"sourceType":"datasetVersion","datasetId":7293788},{"sourceId":11625977,"sourceType":"datasetVersion","datasetId":7293804},{"sourceId":11654479,"sourceType":"datasetVersion","datasetId":7313942},{"sourceId":11706440,"sourceType":"datasetVersion","datasetId":7330450,"isSourceIdPinned":true},{"sourceId":366194,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":303698,"modelId":324180}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Add FPTrans with VISION24 dataset","metadata":{}},{"cell_type":"code","source":"! rm /kaggle/working/FPTrans/data/pretrained/vit/B_16-i1k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setup VISION24","metadata":{}},{"cell_type":"code","source":"! pip install sacred\n!pip install dropblock\n! cp -r /kaggle/input/fptrans/pytorch/default/1/FPTrans /kaggle/working/\n! cp -r /kaggle/input/vision24/VISION24 /kaggle/working/FPTrans/data/\n\n%cd /kaggle/working/FPTrans\n! rm -rf /kaggle/working/FPTrans/data/VISION24/weights\n! rm /kaggle/working/FPTrans/lists/vision24/test_0.pkl\n! rm /kaggle/working/FPTrans/lists/vision24/train_0.pkl\n! cp /kaggle/input/vision24/test.txt  /kaggle/working/FPTrans/lists/vision24\n! cp /kaggle/input/vision24/train.txt  /kaggle/working/FPTrans/lists/vision24\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:55:14.503528Z","iopub.execute_input":"2025-05-06T21:55:14.503696Z","iopub.status.idle":"2025-05-06T21:56:58.002713Z","shell.execute_reply.started":"2025-05-06T21:55:14.503680Z","shell.execute_reply":"2025-05-06T21:56:58.001912Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Collecting sacred\n  Downloading sacred-0.8.7-py2.py3-none-any.whl.metadata (13 kB)\nCollecting docopt-ng<1.0,>=0.9 (from sacred)\n  Downloading docopt_ng-0.9.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: jsonpickle>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from sacred) (4.0.1)\nCollecting munch<5.0,>=2.5 (from sacred)\n  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: wrapt<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from sacred) (1.17.2)\nRequirement already satisfied: py-cpuinfo>=4.0 in /usr/local/lib/python3.11/dist-packages (from sacred) (9.0.0)\nRequirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from sacred) (0.4.6)\nRequirement already satisfied: packaging>=18.0 in /usr/local/lib/python3.11/dist-packages (from sacred) (24.2)\nRequirement already satisfied: GitPython in /usr/local/lib/python3.11/dist-packages (from sacred) (3.1.44)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython->sacred) (4.0.12)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython->sacred) (5.0.2)\nDownloading sacred-0.8.7-py2.py3-none-any.whl (108 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.2/108.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docopt_ng-0.9.0-py3-none-any.whl (16 kB)\nDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\nInstalling collected packages: munch, docopt-ng, sacred\nSuccessfully installed docopt-ng-0.9.0 munch-4.0.0 sacred-0.8.7\nCollecting dropblock\n  Downloading dropblock-0.3.0-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from dropblock) (1.26.4)\nRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from dropblock) (2.5.1+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=0.4.1->dropblock)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=0.4.1->dropblock)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=0.4.1->dropblock)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=0.4.1->dropblock)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=0.4.1->dropblock)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=0.4.1->dropblock)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=0.4.1->dropblock)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.1->dropblock) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=0.4.1->dropblock) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->dropblock) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->dropblock) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->dropblock) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->dropblock) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->dropblock) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->dropblock) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=0.4.1->dropblock) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->dropblock) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->dropblock) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->dropblock) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->dropblock) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->dropblock) (2024.2.0)\nDownloading dropblock-0.3.0-py3-none-any.whl (5.4 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, dropblock\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dropblock-0.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n/kaggle/working/FPTrans\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!rm -rf /kaggle/working/FPTrans/data/VISION24/weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:21:41.683125Z","iopub.execute_input":"2025-05-01T23:21:41.683416Z","iopub.status.idle":"2025-05-01T23:21:41.800628Z","shell.execute_reply.started":"2025-05-01T23:21:41.683391Z","shell.execute_reply":"2025-05-01T23:21:41.799880Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/FPTrans/output/7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:58:07.821249Z","iopub.execute_input":"2025-05-06T21:58:07.821507Z","iopub.status.idle":"2025-05-06T21:58:07.938525Z","shell.execute_reply.started":"2025-05-06T21:58:07.821487Z","shell.execute_reply":"2025-05-06T21:58:07.937547Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!cp /kaggle/input/d/wasimatta/newpath/bestckpt.pth /kaggle/working/FPTrans/output/7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:58:09.181187Z","iopub.execute_input":"2025-05-06T21:58:09.181762Z","iopub.status.idle":"2025-05-06T21:58:13.854802Z","shell.execute_reply.started":"2025-05-06T21:58:09.181706Z","shell.execute_reply":"2025-05-06T21:58:13.853980Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir /kaggle/working/FPTrans/data/pretrained/deit/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:58:17.226395Z","iopub.execute_input":"2025-05-06T21:58:17.226665Z","iopub.status.idle":"2025-05-06T21:58:17.343969Z","shell.execute_reply.started":"2025-05-06T21:58:17.226639Z","shell.execute_reply":"2025-05-06T21:58:17.343199Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"!cp /kaggle/input/deit-backbone/deit_base_distilled_patch16_384-d0272ac0.pth /kaggle/working/FPTrans/data/pretrained/deit/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:58:20.857646Z","iopub.execute_input":"2025-05-06T21:58:20.857934Z","iopub.status.idle":"2025-05-06T21:58:25.006670Z","shell.execute_reply.started":"2025-05-06T21:58:20.857910Z","shell.execute_reply":"2025-05-06T21:58:25.005935Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"! python tools.py precompute_loss_weights with dataset=VISION24","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:58:28.748454Z","iopub.execute_input":"2025-05-06T21:58:28.749023Z","iopub.status.idle":"2025-05-06T22:05:31.886019Z","shell.execute_reply.started":"2025-05-06T21:58:28.748996Z","shell.execute_reply":"2025-05-06T22:05:31.885233Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/FPTrans/data/VISION24/weights/000149.npz: 100%|█| 1616/1616 [06:\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport matplotlib.pyplot as plt\n\ndef plot_training_metrics(log_file):\n    with open(log_file, 'r') as f:\n        lines = f.readlines()\n\n    # Regular expression to match log lines with metrics\n    pattern = r'\\[(?P<epoch>\\d+)/\\d+\\] \\| Tr (?P<train_loss>\\d+\\.\\d+) \\| Val (?P<val_loss>\\d+\\.\\d+) \\| mIoU \\s*(?P<mIoU>\\d+\\.\\d+) \\| bIoU \\s*(?P<bIoU>\\d+\\.\\d+) \\|.*?Catch Rate (?P<catch_rate>\\d+\\.\\d+)'\n\n    train_losses = []\n    val_losses = []\n    catch_rates = []\n\n    # Parse each line\n    for line in lines:\n        match = re.search(pattern, line)\n        if match:\n            train_loss = float(match.group('train_loss'))\n            val_loss = float(match.group('val_loss'))\n            catch_rate = float(match.group('catch_rate'))\n            train_losses.append(train_loss)\n            val_losses.append(val_loss)\n            catch_rates.append(catch_rate)\n\n    if not train_losses:\n        print(\"No metrics found in the log file.\")\n        return\n\n    # Generate epochs list\n    epochs = range(1, len(train_losses) + 1)\n\n    # Create plots\n    plt.figure(figsize=(12, 6))\n\n    # Training Loss Plot\n    plt.subplot(1, 3, 1)\n    plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # Validation Loss Plot\n    plt.subplot(1, 3, 2)\n    plt.plot(epochs, val_losses, label='Val Loss', color='orange')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Validation Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # Catch Rate Plot\n    plt.subplot(1, 3, 3)\n    plt.plot(epochs, catch_rates, label='Catch Rate', color='green')\n    plt.xlabel('Epoch')\n    plt.ylabel('Catch Rate')\n    plt.title('Catch Rate')\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:06:25.571263Z","iopub.execute_input":"2025-05-06T22:06:25.571862Z","iopub.status.idle":"2025-05-06T22:06:25.579659Z","shell.execute_reply.started":"2025-05-06T22:06:25.571835Z","shell.execute_reply":"2025-05-06T22:06:25.578893Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Run training and save logs\n! python run.py train with split=0 configs/vision24_vit.yml > training_log.txt\n\n# Plot the metrics\nplot_training_metrics('training_log.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:09:47.157544Z","iopub.execute_input":"2025-05-06T22:09:47.158273Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"2025-05-06 22:09:51: I Running command 'train'\n2025-05-06 22:09:51: I Started run with ID \"10\"\n2025-05-06 22:09:51: I RUN DIRECTORY: /kaggle/working/FPTrans/output/10\n2025-05-06 22:09:51: I Run:run.py train with split=0 configs/vision24_vit.yml\n2025-05-06 22:09:51: I Init ==> split 0, shot 1\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n2025-05-06 22:09:51: I      ==> Data loader VISION24 for train\n2025-05-06 22:09:51: I      ==> Data loader VISION24 for eval_online\n2025-05-06 22:09:51: I      ==> 6400 training samples\n2025-05-06 22:09:51: I      ==> 400 eval_online samples\n/kaggle/working/FPTrans/networks/vit.py:586: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load(checkpoint_path, map_location='cpu')['model']\n2025-05-06 22:09:52: I      ==> 151 parameters loaded.\n2025-05-06 22:09:52: I      ==> DeiT-B/16-384 initialized from /kaggle/working/FPTrans/data/pretrained/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n2025-05-06 22:09:53: I      ==> 151 parameters loaded.\n2025-05-06 22:09:53: I      ==> DeiT-B/16-384 initialized from /kaggle/working/FPTrans/data/pretrained/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n2025-05-06 22:09:53: I      ==> IntraImageContrastLoss is used.\n2025-05-06 22:09:53: I      ==> Model FPTrans/DeiT-B/16-384 created\n2025-05-06 22:09:54: I      ==> CELossWithDT is used.\n2025-05-06 22:09:54: I Number of trainable parameters: 158\n2025-05-06 22:09:54: I      ==> CrossEntropyLoss is used.\n2025-05-06 22:09:54: I Start training.\n2025-05-06 22:09:54: I Learning rate for ADAMW param_group[0] is 0.0001\n  0%|          | 0/800 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[TRAIN] [1/15] loss: 0.0408 prompt: 0.0319 pair: 0.0012 :  77%|███████▋  | 617/800 [29:40<08:55,  2.93s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"! python run.py train with split=0 configs/vision24_vit.yml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:00:06.762885Z","iopub.execute_input":"2025-05-04T19:00:06.763646Z","iopub.status.idle":"2025-05-04T23:24:35.329989Z","shell.execute_reply.started":"2025-05-04T19:00:06.763601Z","shell.execute_reply":"2025-05-04T23:24:35.329206Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"2025-05-04 19:00:11: I Running command 'train'\n2025-05-04 19:00:11: I Started run with ID \"5\"\n2025-05-04 19:00:11: I RUN DIRECTORY: /kaggle/working/FPTrans/output/5\n2025-05-04 19:00:11: I Run:run.py train with split=0 configs/vision24_vit.yml\n2025-05-04 19:00:11: I Init ==> split 0, shot 1\nbegin\nProcessing data...\n100%|██████████| 1297/1297 [00:44<00:00, 28.91it/s]\nChecking image&label list done! There are 1297 images in split 0.\nClass 0: 248 images\nClass 1: 200 images\nClass 2: 96 images\nClass 3: 120 images\nClass 4: 104 images\nClass 5: 0 images\nClass 6: 0 images\nClass 7: 0 images\nClass 8: 0 images\nClass 9: 0 images\nClass 10: 448 images\nClass 11: 81 images\nData list length: 1297\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n2025-05-04 19:00:56: I      ==> Data loader VISION24 for train\nbegin\nProcessing data...\n100%|██████████| 327/327 [00:10<00:00, 30.44it/s]\nChecking image&label list done! There are 327 images in split 0.\nClass 0: 62 images\nClass 1: 50 images\nClass 2: 26 images\nClass 3: 31 images\nClass 4: 26 images\nClass 5: 0 images\nClass 6: 0 images\nClass 7: 0 images\nClass 8: 0 images\nClass 9: 0 images\nClass 10: 112 images\nClass 11: 20 images\nData list length: 327\n2025-05-04 19:01:07: I      ==> Data loader VISION24 for eval_online\n2025-05-04 19:01:07: I      ==> 6400 training samples\n2025-05-04 19:01:07: I      ==> 400 eval_online samples\n/kaggle/working/FPTrans/networks/vit.py:586: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load(checkpoint_path, map_location='cpu')['model']\n2025-05-04 19:01:07: I      ==> 103 parameters loaded.\n2025-05-04 19:01:07: I      ==> DeiT-B/16-384 initialized from /kaggle/working/FPTrans/data/pretrained/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n2025-05-04 19:01:08: I      ==> 103 parameters loaded.\n2025-05-04 19:01:08: I      ==> DeiT-B/16-384 initialized from /kaggle/working/FPTrans/data/pretrained/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n2025-05-04 19:01:08: I      ==> IntraImageContrastLoss is used.\n2025-05-04 19:01:08: I      ==> Model FPTrans/DeiT-B/16-384 created\n2025-05-04 19:01:09: I      ==> CELossWithDT is used.\n2025-05-04 19:01:09: I Number of trainable parameters: 110\n2025-05-04 19:01:09: I      ==> CrossEntropyLoss is used.\n2025-05-04 19:01:09: I Start training.\n2025-05-04 19:01:09: I Learning rate for ADAMW param_group[0] is 0.0001\n  0%|          | 0/400 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[TRAIN] [1/10] loss: 0.0369 prompt: 0.0292 pair: 0.0015 : 100%|██████████| 400/400 [25:34<00:00,  3.84s/it]\n[EVAL_ONLINE] [1/10] loss: 0.09920: 100%|██████████| 400/400 [00:55<00:00,  7.21it/s]\n/kaggle/working/FPTrans/core/metrics.py:101: RuntimeWarning: invalid value encountered in scalar divide\n  yeild_rate = (correct_yeild) / (negative_pairs)\n2025-05-04 19:27:39: I mIoU mean: [ 5.07] ==>  5.07\n2025-05-04 19:27:39: I bIoU mean: [ 5.07] ==>  5.07\n2025-05-04 19:27:40: I [1/10] | Tr 0.0369 | Val 0.0992 | mIoU  5.07 | bIoU  5.07 | DATA 00:00:01 | OPT 00:25:38 | ETA 03:50:45 | Catch Rate 21.25 | Yeild Rate   nan | (best)\n2025-05-04 19:27:40: I \u001b[92m[1/10] Best mIoU until now: 5.07\n\u001b[0m\n2025-05-04 19:27:40: I Learning rate for ADAMW param_group[0] is 9.75528e-05\n[TRAIN] [2/10] loss: 0.0236 prompt: 0.0199 pair: 0.0012 : 100%|██████████| 400/400 [25:16<00:00,  3.79s/it]\n[EVAL_ONLINE] [2/10] loss: 0.09715: 100%|██████████| 400/400 [00:55<00:00,  7.23it/s]\n2025-05-04 19:53:52: I mIoU mean: [ 5.21] ==>  5.21\n2025-05-04 19:53:52: I bIoU mean: [ 5.21] ==>  5.21\n2025-05-04 19:53:53: I [2/10] | Tr 0.0236 | Val 0.0972 | mIoU  5.21 | bIoU  5.21 | DATA 00:00:02 | OPT 00:25:31 | ETA 03:24:13 | Catch Rate 18.25 | Yeild Rate   nan | (best)\n2025-05-04 19:53:53: I \u001b[92m[2/10] Best mIoU until now: 5.21\n\u001b[0m\n2025-05-04 19:53:53: I Learning rate for ADAMW param_group[0] is 9.04508e-05\n[TRAIN] [3/10] loss: 0.0219 prompt: 0.0186 pair: 0.0013 : 100%|██████████| 400/400 [25:14<00:00,  3.79s/it]\n[EVAL_ONLINE] [3/10] loss: 0.10348: 100%|██████████| 400/400 [00:55<00:00,  7.21it/s]\n2025-05-04 20:20:04: I mIoU mean: [ 4.96] ==>  4.96\n2025-05-04 20:20:04: I bIoU mean: [ 4.96] ==>  4.96\n2025-05-04 20:20:04: I [3/10] | Tr 0.0219 | Val 0.1035 | mIoU  4.96 | bIoU  4.96 | DATA 00:00:02 | OPT 00:25:30 | ETA 02:58:36 | Catch Rate 19.00 | Yeild Rate   nan\n2025-05-04 20:20:04: I \u001b[92m[3/10] Best mIoU until now: 5.21\n\u001b[0m\n2025-05-04 20:20:04: I Learning rate for ADAMW param_group[0] is 7.93893e-05\n[TRAIN] [4/10] loss: 0.0205 prompt: 0.0174 pair: 0.0011 : 100%|██████████| 400/400 [25:17<00:00,  3.79s/it]\n[EVAL_ONLINE] [4/10] loss: 0.07669: 100%|██████████| 400/400 [00:55<00:00,  7.27it/s]\n2025-05-04 20:46:17: I mIoU mean: [ 6.77] ==>  6.77\n2025-05-04 20:46:17: I bIoU mean: [ 6.77] ==>  6.77\n2025-05-04 20:46:18: I [4/10] | Tr 0.0205 | Val 0.0767 | mIoU  6.77 | bIoU  6.77 | DATA 00:00:01 | OPT 00:25:30 | ETA 02:33:03 | Catch Rate 22.25 | Yeild Rate   nan | (best)\n2025-05-04 20:46:18: I \u001b[92m[4/10] Best mIoU until now: 6.77\n\u001b[0m\n2025-05-04 20:46:18: I Learning rate for ADAMW param_group[0] is 6.54508e-05\n[TRAIN] [5/10] loss: 0.0164 prompt: 0.0142 pair: 0.0010 : 100%|██████████| 400/400 [25:15<00:00,  3.79s/it]\n[EVAL_ONLINE] [5/10] loss: 0.06602: 100%|██████████| 400/400 [00:55<00:00,  7.27it/s]\n2025-05-04 21:12:28: I mIoU mean: [ 7.50] ==>  7.50\n2025-05-04 21:12:28: I bIoU mean: [ 7.50] ==>  7.50\n2025-05-04 21:12:29: I [5/10] | Tr 0.0164 | Val 0.0660 | mIoU  7.50 | bIoU  7.50 | DATA 00:00:01 | OPT 00:25:30 | ETA 02:07:30 | Catch Rate 22.50 | Yeild Rate   nan | (best)\n2025-05-04 21:12:29: I \u001b[92m[5/10] Best mIoU until now: 7.50\n\u001b[0m\n2025-05-04 21:12:29: I Learning rate for ADAMW param_group[0] is 5e-05\n[TRAIN] [6/10] loss: 0.0140 prompt: 0.0122 pair: 0.0009 : 100%|██████████| 400/400 [25:17<00:00,  3.79s/it]\n[EVAL_ONLINE] [6/10] loss: 0.06126: 100%|██████████| 400/400 [00:55<00:00,  7.26it/s]\n2025-05-04 21:38:42: I mIoU mean: [ 7.87] ==>  7.87\n2025-05-04 21:38:42: I bIoU mean: [ 7.87] ==>  7.87\n2025-05-04 21:38:43: I [6/10] | Tr 0.0140 | Val 0.0613 | mIoU  7.87 | bIoU  7.87 | DATA 00:00:02 | OPT 00:25:30 | ETA 01:42:00 | Catch Rate 23.00 | Yeild Rate   nan | (best)\n2025-05-04 21:38:43: I \u001b[92m[6/10] Best mIoU until now: 7.87\n\u001b[0m\n2025-05-04 21:38:43: I Learning rate for ADAMW param_group[0] is 3.45492e-05\n[TRAIN] [7/10] loss: 0.0127 prompt: 0.0113 pair: 0.0008 : 100%|██████████| 400/400 [25:19<00:00,  3.80s/it]\n[EVAL_ONLINE] [7/10] loss: 0.05802: 100%|██████████| 400/400 [00:55<00:00,  7.25it/s]\n2025-05-04 22:04:57: I mIoU mean: [ 8.48] ==>  8.48\n2025-05-04 22:04:57: I bIoU mean: [ 8.48] ==>  8.48\n2025-05-04 22:04:58: I [7/10] | Tr 0.0127 | Val 0.0580 | mIoU  8.48 | bIoU  8.48 | DATA 00:00:01 | OPT 00:25:31 | ETA 01:16:34 | Catch Rate 24.00 | Yeild Rate   nan | (best)\n2025-05-04 22:04:58: I \u001b[92m[7/10] Best mIoU until now: 8.48\n\u001b[0m\n2025-05-04 22:04:58: I Learning rate for ADAMW param_group[0] is 2.06107e-05\n[TRAIN] [8/10] loss: 0.0120 prompt: 0.0108 pair: 0.0008 : 100%|██████████| 400/400 [25:18<00:00,  3.80s/it]\n[EVAL_ONLINE] [8/10] loss: 0.05797: 100%|██████████| 400/400 [00:55<00:00,  7.27it/s]\n2025-05-04 22:31:11: I mIoU mean: [ 8.85] ==>  8.85\n2025-05-04 22:31:11: I bIoU mean: [ 8.85] ==>  8.85\n2025-05-04 22:31:12: I [8/10] | Tr 0.0120 | Val 0.0580 | mIoU  8.85 | bIoU  8.85 | DATA 00:00:01 | OPT 00:25:31 | ETA 00:51:03 | Catch Rate 25.00 | Yeild Rate   nan | (best)\n2025-05-04 22:31:12: I \u001b[92m[8/10] Best mIoU until now: 8.85\n\u001b[0m\n2025-05-04 22:31:12: I Learning rate for ADAMW param_group[0] is 9.54915e-06\n[TRAIN] [9/10] loss: 0.0113 prompt: 0.0101 pair: 0.0007 : 100%|██████████| 400/400 [25:14<00:00,  3.79s/it]\n[EVAL_ONLINE] [9/10] loss: 0.05853: 100%|██████████| 400/400 [00:55<00:00,  7.26it/s]\n2025-05-04 22:57:22: I mIoU mean: [ 8.82] ==>  8.82\n2025-05-04 22:57:22: I bIoU mean: [ 8.82] ==>  8.82\n2025-05-04 22:57:23: I [9/10] | Tr 0.0113 | Val 0.0585 | mIoU  8.82 | bIoU  8.82 | DATA 00:00:02 | OPT 00:25:31 | ETA 00:25:31 | Catch Rate 24.25 | Yeild Rate   nan\n2025-05-04 22:57:23: I \u001b[92m[9/10] Best mIoU until now: 8.85\n\u001b[0m\n2025-05-04 22:57:23: I Learning rate for ADAMW param_group[0] is 2.44717e-06\n[TRAIN] [10/10] loss: 0.0111 prompt: 0.0101 pair: 0.0007 : 100%|██████████| 400/400 [25:15<00:00,  3.79s/it]\n[EVAL_ONLINE] [10/10] loss: 0.05704: 100%|██████████| 400/400 [00:55<00:00,  7.25it/s]\n2025-05-04 23:23:34: I mIoU mean: [ 9.00] ==>  9.00\n2025-05-04 23:23:34: I bIoU mean: [ 9.00] ==>  9.00\n2025-05-04 23:23:35: I [10/10] | Tr 0.0111 | Val 0.0570 | mIoU  9.00 | bIoU  9.00 | DATA 00:00:01 | OPT 00:25:32 | ETA 00:00:00 | Catch Rate 24.25 | Yeild Rate   nan | (best)\n2025-05-04 23:23:35: I \u001b[92m[10/10] Best mIoU until now: 9.00\n\u001b[0m\n2025-05-04 23:23:35: I ============ Training finished - id 5 ============\n\n2025-05-04 23:23:35: I Run:run.py train with split=0 configs/vision24_vit.yml\nbegin\n2025-05-04 23:23:35: I Init ==> split 0, shot 1\nClass 0: 62 images\nClass 1: 50 images\nClass 2: 26 images\nClass 3: 31 images\nClass 4: 26 images\nClass 5: 0 images\nClass 6: 0 images\nClass 7: 0 images\nClass 8: 0 images\nClass 9: 0 images\nClass 10: 112 images\nClass 11: 20 images\nData list length: 327\n2025-05-04 23:23:35: I      ==> Data loader VISION24 for test\n2025-05-04 23:23:35: I      ==> 400 testing samples\n2025-05-04 23:23:35: I      ==> 103 parameters loaded.\n2025-05-04 23:23:35: I      ==> DeiT-B/16-384 initialized from /kaggle/working/FPTrans/data/pretrained/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n2025-05-04 23:23:36: I      ==> 103 parameters loaded.\n2025-05-04 23:23:36: I      ==> DeiT-B/16-384 initialized from /kaggle/working/FPTrans/data/pretrained/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n2025-05-04 23:23:36: I      ==> IntraImageContrastLoss is used.\n2025-05-04 23:23:36: I      ==> Model FPTrans/DeiT-B/16-384 created\n2025-05-04 23:23:36: I      ==> Try to load checkpoint from /kaggle/working/FPTrans/output/5/bestckpt.pth\n/kaggle/working/FPTrans/networks/FPTrans.py:320: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weights = torch.load(str(ckpt_path), map_location='cpu')\n2025-05-04 23:23:36: I      ==> Model FPTrans/DeiT-B/16-384 initialized from /kaggle/working/FPTrans/output/5/bestckpt.pth\n2025-05-04 23:23:36: I      ==> Checkpoint loaded.\n2025-05-04 23:23:36: I      ==> CrossEntropyLoss is used.\n2025-05-04 23:23:36: I Start testing.\n[EVAL] loss: 0.05704: 100%|██████████| 400/400 [00:55<00:00,  7.25it/s]\n2025-05-04 23:24:31: I ╒═══════ Final Results ════════╕\n2025-05-04 23:24:31: I │ mIoU mean: [ 9.00] ==>  9.00 │\n2025-05-04 23:24:31: I │ bIoU mean: [ 9.00] ==>  9.00 │\n2025-05-04 23:24:31: I │ speed: 10.06 FPS             │\n2025-05-04 23:24:31: I ╘══════════════════════════════╛\n2025-05-04 23:24:31: I Result: Loss: 0.0570, mIoU: 9.00, bIoU: 9.00 , catch_rate:24.00, yeild_rate: nan\n2025-05-04 23:24:31: I Completed after 4:24:21\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"pip install timm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:07:13.131504Z","iopub.execute_input":"2025-05-06T22:07:13.132247Z","iopub.status.idle":"2025-05-06T22:07:16.131046Z","shell.execute_reply.started":"2025-05-06T22:07:13.132213Z","shell.execute_reply":"2025-05-06T22:07:16.130216Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.14)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.20.1+cu124)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->timm) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->timm) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"! python run.py train with split=0 exp_id=7 configs/vision24_vit.yml > training_log.txt\n\n# Plot the metrics\nplot_training_metrics('training_log.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:07:19.628926Z","iopub.execute_input":"2025-05-06T22:07:19.629198Z","iopub.status.idle":"2025-05-06T22:09:03.532758Z","shell.execute_reply.started":"2025-05-06T22:07:19.629174Z","shell.execute_reply":"2025-05-06T22:09:03.531886Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"2025-05-06 22:07:23: I Running command 'train'\n2025-05-06 22:07:24: I Started run with ID \"9\"\n2025-05-06 22:07:24: I RUN DIRECTORY: /kaggle/working/FPTrans/output/9\n2025-05-06 22:07:24: I Run:run.py train with split=0 exp_id=7 configs/vision24_vit.yml\n2025-05-06 22:07:24: I Init ==> split 0, shot 1\n100%|██████████| 1616/1616 [01:17<00:00, 20.79it/s]\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n2025-05-06 22:08:41: I      ==> Data loader VISION24 for train\n100%|██████████| 408/408 [00:18<00:00, 21.74it/s]\n2025-05-06 22:09:00: I      ==> Data loader VISION24 for eval_online\n2025-05-06 22:09:00: I      ==> 6400 training samples\n2025-05-06 22:09:00: I      ==> 400 eval_online samples\n/kaggle/working/FPTrans/networks/vit.py:586: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load(checkpoint_path, map_location='cpu')['model']\n2025-05-06 22:09:01: I      ==> 151 parameters loaded.\n2025-05-06 22:09:01: I      ==> DeiT-B/16-384 initialized from /kaggle/working/FPTrans/data/pretrained/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n2025-05-06 22:09:02: I      ==> 151 parameters loaded.\n2025-05-06 22:09:02: I      ==> DeiT-B/16-384 initialized from /kaggle/working/FPTrans/data/pretrained/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n2025-05-06 22:09:02: I      ==> IntraImageContrastLoss is used.\n2025-05-06 22:09:02: I      ==> Model FPTrans/DeiT-B/16-384 created\n/kaggle/working/FPTrans/networks/FPTrans.py:320: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weights = torch.load(str(ckpt_path), map_location='cpu')\n2025-05-06 22:09:02: E Failed after 0:01:39!\nTraceback (most recent calls WITHOUT Sacred internals):\n  File \"/kaggle/working/FPTrans/run.py\", line 95, in train\n    model.load_weights(ckpt, logger, strict=opt.strict)\n  File \"/kaggle/working/FPTrans/networks/FPTrans.py\", line 329, in load_weights\n    self.load_state_dict(weights, strict=strict)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 2584, in load_state_dict\n    raise RuntimeError(\nRuntimeError: Error(s) in loading state_dict for FPTrans/DeiT-B/16-384:\n\tsize mismatch for encoder.backbone.prompt_tokens: copying a param with shape torch.Size([1560, 12, 768]) from checkpoint, the shape in current model is torch.Size([120, 12, 768]).\n\tsize mismatch for encoder.backbone.pos_embed: copying a param with shape torch.Size([1, 902, 768]) from checkpoint, the shape in current model is torch.Size([1, 578, 768]).\n\nNo metrics found in the log file.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"! python run.py train with split=0 exp_id=7 configs/vision24_vit.yml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T04:56:05.923349Z","iopub.execute_input":"2025-05-05T04:56:05.924116Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"2025-05-05 04:56:10: I Running command 'train'\n2025-05-05 04:56:11: I Started run with ID \"8\"\n2025-05-05 04:56:11: I RUN DIRECTORY: /kaggle/working/FPTrans/output/8\n2025-05-05 04:56:11: I Run:run.py train with split=0 exp_id=7 configs/vision24_vit.yml\n2025-05-05 04:56:11: I Init ==> split 0, shot 1\nbegin\nClass 0: 248 images\nClass 1: 200 images\nClass 2: 96 images\nClass 3: 120 images\nClass 4: 104 images\nClass 5: 0 images\nClass 6: 0 images\nClass 7: 0 images\nClass 8: 0 images\nClass 9: 0 images\nClass 10: 448 images\nClass 11: 81 images\nData list length: 1297\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n2025-05-05 04:56:11: I      ==> Data loader VISION24 for train\nbegin\nClass 0: 62 images\nClass 1: 50 images\nClass 2: 26 images\nClass 3: 31 images\nClass 4: 26 images\nClass 5: 0 images\nClass 6: 0 images\nClass 7: 0 images\nClass 8: 0 images\nClass 9: 0 images\nClass 10: 112 images\nClass 11: 20 images\nData list length: 327\n2025-05-05 04:56:11: I      ==> Data loader VISION24 for eval_online\n2025-05-05 04:56:11: I      ==> 6400 training samples\n2025-05-05 04:56:11: I      ==> 400 eval_online samples\n/kaggle/working/FPTrans/networks/vit.py:586: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ckpt = torch.load(checkpoint_path, map_location='cpu')['model']\n2025-05-05 04:56:11: I      ==> 103 parameters loaded.\n2025-05-05 04:56:11: I      ==> DeiT-B/16-384 initialized from /kaggle/working/FPTrans/data/pretrained/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n2025-05-05 04:56:12: I      ==> 103 parameters loaded.\n2025-05-05 04:56:12: I      ==> DeiT-B/16-384 initialized from /kaggle/working/FPTrans/data/pretrained/deit/deit_base_distilled_patch16_384-d0272ac0.pth\n2025-05-05 04:56:12: I      ==> IntraImageContrastLoss is used.\n2025-05-05 04:56:12: I      ==> Model FPTrans/DeiT-B/16-384 created\n/kaggle/working/FPTrans/networks/FPTrans.py:320: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  weights = torch.load(str(ckpt_path), map_location='cpu')\n2025-05-05 04:56:12: I      ==> Model FPTrans/DeiT-B/16-384 initialized from /kaggle/working/FPTrans/output/7/bestckpt.pth\n2025-05-05 04:56:13: I      ==> CELossWithDT is used.\n2025-05-05 04:56:13: I Number of trainable parameters: 110\n2025-05-05 04:56:13: I      ==> CrossEntropyLoss is used.\n2025-05-05 04:56:13: I Start training.\n2025-05-05 04:56:13: I Learning rate for ADAMW param_group[0] is 0.0001\n  0%|          | 0/400 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n[TRAIN] [1/15] loss: 0.0152 prompt: 0.0132 pair: 0.0009 : 100%|██████████| 400/400 [27:57<00:00,  4.19s/it]\n[EVAL_ONLINE] [1/15] loss: 0.06035: 100%|██████████| 400/400 [01:00<00:00,  6.64it/s]\n/kaggle/working/FPTrans/core/metrics.py:101: RuntimeWarning: invalid value encountered in scalar divide\n  yeild_rate = (correct_yeild) / (negative_pairs)\n2025-05-05 05:25:10: I mIoU mean: [ 8.47] ==>  8.47\n2025-05-05 05:25:10: I bIoU mean: [ 8.47] ==>  8.47\n2025-05-05 05:25:11: I [1/15] | Tr 0.0152 | Val 0.0603 | mIoU  8.47 | bIoU  8.47 | DATA 00:00:01 | OPT 00:28:02 | ETA 06:32:34 | Catch Rate 24.50 | Yeild Rate   nan | (best)\n2025-05-05 05:25:11: I \u001b[92m[1/15] Best mIoU until now: 8.47\n\u001b[0m\n2025-05-05 05:25:11: I Learning rate for ADAMW param_group[0] is 9.89074e-05\n[TRAIN] [2/15] loss: 0.0143 prompt: 0.0126 pair: 0.0009 : 100%|██████████| 400/400 [27:07<00:00,  4.07s/it]\n[EVAL_ONLINE] [2/15] loss: 0.07347: 100%|██████████| 400/400 [01:00<00:00,  6.66it/s]\n2025-05-05 05:53:19: I mIoU mean: [ 6.94] ==>  6.94\n2025-05-05 05:53:19: I bIoU mean: [ 6.94] ==>  6.94\n2025-05-05 05:53:19: I [2/15] | Tr 0.0143 | Val 0.0735 | mIoU  6.94 | bIoU  6.94 | DATA 00:00:01 | OPT 00:27:24 | ETA 05:56:14 | Catch Rate 22.50 | Yeild Rate   nan\n2025-05-05 05:53:19: I \u001b[92m[2/15] Best mIoU until now: 8.47\n\u001b[0m\n2025-05-05 05:53:19: I Learning rate for ADAMW param_group[0] is 9.56773e-05\n[TRAIN] [3/15] loss: 0.0159 prompt: 0.0137 pair: 0.0010 : 100%|██████████| 400/400 [27:03<00:00,  4.06s/it]\n[EVAL_ONLINE] [3/15] loss: 0.09240: 100%|██████████| 400/400 [00:59<00:00,  6.70it/s]\n2025-05-05 06:21:23: I mIoU mean: [ 5.73] ==>  5.73\n2025-05-05 06:21:23: I bIoU mean: [ 5.73] ==>  5.73\n2025-05-05 06:21:23: I [3/15] | Tr 0.0159 | Val 0.0924 | mIoU  5.73 | bIoU  5.73 | DATA 00:00:03 | OPT 00:27:23 | ETA 05:28:38 | Catch Rate 19.25 | Yeild Rate   nan\n2025-05-05 06:21:23: I \u001b[92m[3/15] Best mIoU until now: 8.47\n\u001b[0m\n2025-05-05 06:21:23: I Learning rate for ADAMW param_group[0] is 9.04508e-05\n[TRAIN] [4/15] loss: 0.0169 prompt: 0.0146 pair: 0.0010 : 100%|██████████| 400/400 [27:08<00:00,  4.07s/it]\n[EVAL_ONLINE] [4/15] loss: 0.09364: 100%|██████████| 400/400 [00:59<00:00,  6.71it/s]\n2025-05-05 06:49:32: I mIoU mean: [ 5.29] ==>  5.29\n2025-05-05 06:49:32: I bIoU mean: [ 5.29] ==>  5.29\n2025-05-05 06:49:32: I [4/15] | Tr 0.0169 | Val 0.0936 | mIoU  5.29 | bIoU  5.29 | DATA 00:00:01 | OPT 00:27:22 | ETA 05:01:11 | Catch Rate 19.25 | Yeild Rate   nan\n2025-05-05 06:49:32: I \u001b[92m[4/15] Best mIoU until now: 8.47\n\u001b[0m\n2025-05-05 06:49:32: I Learning rate for ADAMW param_group[0] is 8.34565e-05\n[TRAIN] [5/15] loss: 0.0133 prompt: 0.0118 pair: 0.0009 : 100%|██████████| 400/400 [27:03<00:00,  4.06s/it]\n[EVAL_ONLINE] [5/15] loss: 0.07355: 100%|██████████| 400/400 [01:00<00:00,  6.64it/s]\n2025-05-05 07:17:36: I mIoU mean: [ 6.73] ==>  6.73\n2025-05-05 07:17:36: I bIoU mean: [ 6.73] ==>  6.73\n2025-05-05 07:17:37: I [5/15] | Tr 0.0133 | Val 0.0736 | mIoU  6.73 | bIoU  6.73 | DATA 00:00:02 | OPT 00:27:23 | ETA 04:33:59 | Catch Rate 22.00 | Yeild Rate   nan\n2025-05-05 07:17:37: I \u001b[92m[5/15] Best mIoU until now: 8.47\n\u001b[0m\n2025-05-05 07:17:37: I Learning rate for ADAMW param_group[0] is 7.5e-05\n[TRAIN] [6/15] loss: 0.0129 prompt: 0.0115 pair: 0.0008 : 100%|██████████| 400/400 [27:03<00:00,  4.06s/it]\n[EVAL_ONLINE] [6/15] loss: 0.05730: 100%|██████████| 400/400 [01:01<00:00,  6.52it/s]\n2025-05-05 07:45:42: I mIoU mean: [ 8.14] ==>  8.14\n2025-05-05 07:45:42: I bIoU mean: [ 8.14] ==>  8.14\n2025-05-05 07:45:42: I [6/15] | Tr 0.0129 | Val 0.0573 | mIoU  8.14 | bIoU  8.14 | DATA 00:00:02 | OPT 00:27:22 | ETA 04:06:19 | Catch Rate 22.50 | Yeild Rate   nan\n2025-05-05 07:45:42: I \u001b[92m[6/15] Best mIoU until now: 8.47\n\u001b[0m\n2025-05-05 07:45:42: I Learning rate for ADAMW param_group[0] is 6.54508e-05\n[TRAIN] [7/15] loss: 0.0117 prompt: 0.0105 pair: 0.0008 :  33%|███▎      | 133/400 [09:29<17:33,  3.95s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"! python run.py train with split=0 exp_id = 7 configs/vision24_vit.yml","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! python run.py test with configs/vision24_vit.yml exp_id=7 split=0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! rm /kaggle/working/FPTrans/lists/vision24/test_0.pkl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! mkdir output/7/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T23:21:26.246114Z","iopub.execute_input":"2025-05-01T23:21:26.246396Z","iopub.status.idle":"2025-05-01T23:21:26.364323Z","shell.execute_reply.started":"2025-05-01T23:21:26.246371Z","shell.execute_reply":"2025-05-01T23:21:26.363512Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"! cp /kaggle/input/bestmodel/bestckpt_10epochs.pth /kaggle/working/FPTrans/output/7/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mv /kaggle/working/FPTrans/output/7/bestckpt_10epochs.pth /kaggle/working/FPTrans/output/7/bestckpt.pth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! python run.py train with split=0 configs/vision24_vit.yml","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! python run.py test with configs/vision24_vit.yml exp_id=73 split=0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! python run.py train with split=0 configs/vision24_vit.yml ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! python run.py train with split=0 exp_id=73 configs/vision24_vit.yml ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! python run.py test with configs/vision24_vit.yml exp_id=73 split=0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!find -name \"*000561*\"\n!find -name \"results\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! rm -rf results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! python run.py predict with configs/vision24_vit.yml exp_id=73 split=0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! rm /kaggle/working/FPTrans/lists/vision24/test_0.pkl\n! rm /kaggle/working/FPTrans/lists/vision24/train_0.pkl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/configs/vision24_vit.yml\n# %load /kaggle/working/FPTrans/configs/vision24_vit.yml\ndataset: VISION24\n#backbone: DeiT-B/16-384\nbackbone: DeiT-B/16-384\n\nvit_depth: 12  # DeiT-B/16-384 has 12 transformer layers\nepochs: 15\nlr: 0.0001\nbs: 8\ntrain_n: 6400\ntest_n: 400\nbg_num: 1\nscale_min: 0.6\nscale_max: 1.4\npair_lossW: 0.001\nheight: 384\nwidth: 384\noptim: \"adamw\"\nnetwork: fptrans  # Required by load_model()\n\n\n\n# Checkpoint loading\n#pretrained: true\n# Uncomment and set the correct path to your checkpoint\n# ckpt_path: /kaggle/working/FPTrans/checkpoints/fptrans_deit384_exp40.pth\nstrict: false  # Allows partial loading if minor mismatches persist\n\n# p:\n# #Screw_front\n#   sup_rgb: [\"./data/VISION24/JPEGImages/001757.jpg\",\n           \n#            ]\n#   sup_msk: [\"./data/VISION24/SegmentationClassAug/001757.png\",\n            \n             \n#            ]\n#   qry_rgb: [\"./data/VISION24/JPEGImages/000432.jpg\",\n#            \"./data/VISION24/JPEGImages/000874.jpg\",\n#             \"./data/VISION24/JPEGImages/000114.jpg\",\n#             \"./data/VISION24/JPEGImages/000860.jpg\",\n        \n            \n            \n            \n            \n#            ]\n\n# p:\n# #Cylinder_Porosity\n\n#   sup_rgb: [\"./data/VISION24/JPEGImages/000723.jpg\",\n           \n#            ]\n#   sup_msk: [\"./data/VISION24/SegmentationClassAug/000723.png\",\n            \n             \n#            ]\n#   qry_rgb: [\"./data/VISION24/JPEGImages/000756.jpg\",\n#            \"./data/VISION24/JPEGImages/000742.jpg\",\n#             \"./data/VISION24/JPEGImages/000772.jpg\",\n#             \"./data/VISION24/JPEGImages/000690.jpg\",\n        \n            \n            \n            \n            \n#            ]\n#   out: results/\n\n\n\n# p:\n# #Wood \n\n#   sup_rgb: [\"./data/VISION24/JPEGImages/000377.jpg\",\n           \n#            ]\n#   sup_msk: [\"./data/VISION24/SegmentationClassAug/000377.png\",\n            \n             \n#            ]\n#   qry_rgb: [\"./data/VISION24/JPEGImages/000543.jpg\",\n#            \"./data/VISION24/JPEGImages/000484.jpg\",\n#             \"./data/VISION24/JPEGImages/000485.jpg\",\n#             \"./data/VISION24/JPEGImages/000471.jpg\",        \n#            ]\n#   out: results/\n\n\n# p:\n# #Cable_thunderbolt\n \n\n#   sup_rgb: [\"./data/VISION24/JPEGImages/000175.jpg\",\n           \n#            ]\n#   sup_msk: [\"./data/VISION24/SegmentationClassAug/000175.png\",\n            \n             \n#            ]\n#   qry_rgb: [\"./data/VISION24/JPEGImages/000015.jpg\",\n#            \"./data/VISION24/JPEGImages/000218.jpg\",\n#             \"./data/VISION24/JPEGImages/000167.jpg\",\n#             \"./data/VISION24/JPEGImages/000253.jpg\",        \n#            ]\n#   out: results/\n\n\n# p:\n# #Cylinder_RCS\n\n \n\n#   sup_rgb: [\"./data/VISION24/JPEGImages/000903.jpg\",\n           \n#            ]\n#   sup_msk: [\"./data/VISION24/SegmentationClassAug/000903.png\",\n            \n             \n#            ]\n#   qry_rgb: [\"./data/VISION24/JPEGImages/000860.jpg\",\n#            \"./data/VISION24/JPEGImages/000955.jpg\",\n#             \"./data/VISION24/JPEGImages/000926.jpg\",\n#             \"./data/VISION24/JPEGImages/000878.jpg\",        \n#            ]\n#   out: results/\n\n\np:\n#PCB_spurious_copper\n\n\n \n\n  sup_rgb: [\"./data/VISION24/JPEGImages/001312.jpg\",\n           \n           ]\n  sup_msk: [\"./data/VISION24/SegmentationClassAug/001312.png\",\n            \n             \n           ]\n  qry_rgb: [\"./data/VISION24/JPEGImages/001287.jpg\",\n           \"./data/VISION24/JPEGImages/001302.jpg\",\n            \"./data/VISION24/JPEGImages/001356.jpg\",\n            \"./data/VISION24/JPEGImages/001291.jpg\",        \n           ]\n  out: results/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:53.693810Z","iopub.execute_input":"2025-05-06T21:57:53.694314Z","iopub.status.idle":"2025-05-06T21:57:53.701933Z","shell.execute_reply.started":"2025-05-06T21:57:53.694285Z","shell.execute_reply":"2025-05-06T21:57:53.701197Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/configs/vision24_vit.yml\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/tools.py\n# %load /kaggle/working/FPTrans/tools.py\nfrom pathlib import Path\n\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn.functional as F\nfrom sacred import Experiment\nfrom scipy.ndimage import distance_transform_edt\nimport cv2\n\nfrom config import MapConfig\nfrom data_kits.voc_coco import SemData\nfrom data_kits.datasets import DATA_DIR, DATA_LIST\nfrom utils_.misc import load_image\n\nex = Experiment(\"Tools\", save_git_info=False, base_dir=\"./\")\n\n\n@ex.config\ndef config():\n    dataset = \"PASCAL\"\n    proc = \"cv2\"\n    sigma = 5\n    save_byte = False       # save weights in uint8 format for saving space (especially for COCO)\n    weights_save_dir = DATA_DIR[dataset] / \"weights\"\n    dry_run = False\n\n\ndef boundary2weight(target, cls, kernel, sigma=5):\n    mask = torch.zeros(target.shape, dtype=torch.float32).cuda()\n    mask[target == cls] = 1\n    mask.unsqueeze_(dim=0)      # [1, H, W]\n    mask.unsqueeze_(dim=0)      # [1, 1, H, W]\n\n    # Extract mask boundary (inner and outer)\n    dilated = torch.clamp(F.conv2d(mask, kernel, padding=1), 0, 1) - mask\n    erosion = mask - torch.clamp(F.conv2d(mask, kernel, padding=1) - 8, 0, 1)\n    boundary = (dilated + erosion).squeeze(dim=0).squeeze(dim=0)     # [H, W]\n\n    bool_boundary = np.around(boundary.cpu().numpy()).astype(bool)\n    edt = distance_transform_edt(np.bitwise_not(bool_boundary))\n    weight = np.exp(-edt / sigma ** 2).astype(np.float32)\n    return weight\n\n\n@ex.command(unobserved=True)\ndef precompute_loss_weights(_config):\n    \"\"\"\n\n    Precompute weights for weighted cross-entropy loss.\n\n    Parameters\n    ----------\n    _config: ReadOnlyDict\n        dataset: str\n            Name of the dataset. [PASCAL|COCO | VISION24 ]\n        save_bytes: bool\n            Save weights in float32 or byte. It should be set as True when generating\n            for COCO. Default value is False.\n    Returns\n    -------\n\n    Usage\n    -----\n    cuda 0 python tools.py precompute_loss_weights with dataset=PASCAL\n    cuda 0 python tools.py precompute_loss_weights with dataset=COCO save_bytes=True\n    cuda 0 python tools.py precompute_loss_weights with dataset=VISION24\n\n    \"\"\"\n    opt = MapConfig(_config)\n    save_dir = opt.weights_save_dir\n    save_dir.mkdir(parents=True, exist_ok=True)\n    data_dir = DATA_DIR[opt.dataset]\n\n    # kernel for finding mask boundaries\n    kernel = torch.ones(1, 1, 3, 3, dtype=torch.float).cuda()\n\n    data_list = DATA_LIST[opt.dataset]['train']\n\n    label_paths = [x.split()[1] for x in data_list.read_text().splitlines()]\n\n    #for vision24\n    all_labels = {}\n    if(opt.dataset== \"VISION24\"):\n        all_labels = {str(data_dir/(x.split()[1])):SemData.class_names['vision24'].index(x.split()[2]) for x in data_list.read_text().splitlines()}\n    \n\n    gen = tqdm(label_paths)\n    for lab_path in gen:\n        save_file = save_dir / Path(lab_path).stem\n        gen.set_description(f\"{save_file}.npz\")\n        if save_file.with_suffix('.npz').exists():\n            continue\n\n        classes = []\n        all_class_edts = []\n\n        lab_path = data_dir / lab_path\n        label = load_image(lab_path, 'lab', opt.proc)\n        \n        if(opt.dataset!= \"VISION24\"):\n            unique_labels = np.unique(label).tolist()\n            unique_labels = [1 if x == 255 else x for x in unique_labels]\n        else:\n            unique_labels = all_labels[str(lab_path)]\n            unique_labels = np.unique(unique_labels).tolist()\n                      \n        for cls in unique_labels:\n            if cls == 255 :\n                continue\n            classes.append(cls)\n            all_class_edts.append(boundary2weight(label, cls, kernel, opt.sigma))\n\n        classes = np.array(classes)\n        edt = np.stack(all_class_edts, axis=0)\n        \n        if opt.save_byte:\n            edt = (edt * 255).astype('uint8')\n\n        if not opt.dry_run:\n            np.savez_compressed(save_file, x=edt, c=classes)\n\n\n@ex.command(unobserved=True)\ndef print_ckpt(ckpt):\n    \"\"\"\n\n    This tool helps print the weight names and shapes for inspecting a checkpoint.\n\n    Parameters\n    ----------\n    ckpt: str\n        Path to a checkpoint\n\n    \"\"\"\n    state = torch.load(ckpt, map_location='cpu')\n    if 'model_state' in state:\n        state = state['model_state']\n    elif 'state_dict' in state:\n        state = state['state_dict']\n    elif 'model' in state:\n        state = state['model']\n\n    max_name_length = max([len(x) for x in state])\n    max_shape_length = max([len(str(x.shape)) for x in state.values()])\n    pattern = \"  {:<%ds}  {:<%ds}\" % (max_name_length, max_shape_length)\n\n    print_str = \"\"\n    for k, v in state.items():\n        print_str += pattern.format(k, str(list(v.shape))) + \"\\n\"\n\n    print(print_str)\n\n\n@ex.command(unobserved=True)\ndef gen_coco_labels(sets, _config):\n    \"\"\"\n\n    Generate COCO labels with 'pycocotools' API.\n\n    Parameters\n    ----------\n    sets: str\n        Data sets. The accessible values are [train2014, val2014].\n    _config: ReadOnlyDict\n        dry_run: bool\n            Dry run this command without saving to disk.\n\n    Returns\n    -------\n\n    Usage\n    -----\n    python tools.py gen_coco_labels with sets=train2014\n    python tools.py gen_coco_labels with sets=val2014\n\n    \"\"\"\n    from pycocotools.coco import COCO\n\n    opt = MapConfig(_config)\n    if sets not in ['train2014', 'val2014']:\n        raise ValueError(f'Not supported sets: {sets}. [train2014, val2014]')\n    save_dir = DATA_DIR['COCO'] / f'{sets}_label'\n    annFile = DATA_DIR['COCO'] / f'annotations/instances_{sets}.json'\n    save_dir.mkdir(parents=True, exist_ok=True)\n    print(f'Labels of {sets} are saved to {save_dir}.')\n\n    coco = COCO(str(annFile))\n    # display COCO categories and supercategories\n    cats = coco.loadCats(coco.getCatIds())\n\n    nms = [cat['name'] for cat in cats]\n    num_cats = len(nms)\n    print('All {} categories.'.format(num_cats))\n    print(nms)\n\n    # get all images ids\n    imgIds = coco.getImgIds()\n    gen = tqdm(enumerate(imgIds), total=len(imgIds))\n    for idx, im_id in gen:\n        # load annotations\n        annIds = coco.getAnnIds(imgIds=im_id, iscrowd=False)\n        if len(annIds) == 0:\n            continue\n\n        image = coco.loadImgs([im_id])[0]\n        # image.keys: ['coco_url', 'flickr_url', 'date_captured', 'license', 'width', 'height', 'file_name', 'id']\n        h, w = image['height'], image['width']\n        gt_name = image['file_name'].split('.')[0] + '.png'\n        gt = np.zeros((h, w), dtype=np.uint8)\n\n        # ann.keys: ['area', 'category_id', 'bbox', 'iscrowd', 'id', 'segmentation', 'image_id']\n        anns = coco.loadAnns(annIds)\n        for ann_idx, ann in enumerate(anns):\n\n            cat = coco.loadCats([ann['category_id']])\n            cat = cat[0]['name']\n            cat = nms.index(cat) + 1  # cat_id ranges from 1 to 80\n\n            # below is the original script\n            segs = ann['segmentation']\n            for seg in segs:\n                seg = np.array(seg).reshape(-1, 2)  # [n_points, 2]\n                cv2.fillPoly(gt, [seg.astype(np.int32)], int(cat))\n\n        save_gt_path = save_dir / gt_name\n        gen.set_description(f'{save_gt_path}')\n        if not opt.dry_run:\n            cv2.imwrite(str(save_gt_path), gt)\n\n\nif __name__ == \"__main__\":\n    ex.run_commandline()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:53.707839Z","iopub.execute_input":"2025-05-06T21:57:53.708056Z","iopub.status.idle":"2025-05-06T21:57:53.724987Z","shell.execute_reply.started":"2025-05-06T21:57:53.708036Z","shell.execute_reply":"2025-05-06T21:57:53.724272Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/tools.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/data_kits/voc_coco.py\n# %load  data_kits/voc_coco.py\nimport pickle\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\nimport os\nimport numpy as np\nfrom constants import project_dir, data_dir\nfrom utils_.misc import load_image, load_weights\n\ncache_image = {}\ncache_label = {}\ncache_weights = {}\n\n\ndef make_dataset(split=0, data_root=None, data_list=None, sub_list=None, coco2pascal=False):\n    assert split in [0, 1, 2, 3]\n    data_list = Path(data_list).relative_to(project_dir)\n    if not data_list.is_file():\n        raise RuntimeError(f\"Image list file do not exist: {data_list}\\n\")\n    print('begin')\n    filepath = data_list.parent / f\"{data_list.stem}_{split}{'_coco2pascal' if coco2pascal else ''}.pkl\"\n    if filepath.exists():\n        # Try to load `image_label_list` and `sub_class_file_list` from cache\n        # print(filepath)\n        with filepath.open(\"rb\") as f:\n            image_label_list, sub_class_file_list = pickle.load(f)\n            \n            # print(image_label_list)\n            # print('----------------------------------')\n\n        if len(image_label_list) > 0:\n            return image_label_list, sub_class_file_list\n\n    # Shaban uses these lines to remove small objects:\n    # if util.change_coordinates(mask, 32.0, 0.0).sum() > 2:\n    #    filtered_item.append(item)\n    # which means the mask will be downsampled to 1/32 of the original size and the valid area should be larger than 2,\n    # therefore the area in original size should be accordingly larger than 2 * 32 * 32\n    image_label_list = []\n    list_read = open(data_list).readlines()\n    print(\"Processing data...\".format(sub_list))\n    sub_class_file_list = {}\n\n    # For file name --> if vision24\n    dataset_name = str(filepath).split('/')[-2]\n    for sub_c in sub_list:\n        sub_class_file_list[sub_c] = []\n\n    for l_idx in tqdm(range(len(list_read))):\n        line = list_read[l_idx]\n        line = line.strip()\n        line_split = line.split(' ')\n        image_name = data_root / line_split[0]\n        label_name = data_root / line_split[1]\n        \n        label = cv2.imread(str(label_name), cv2.IMREAD_GRAYSCALE)\n        label_class = np.unique(label).tolist()\n        \n        if(dataset_name == \"vision24\"):\n            #read the label from the file\n            label_class = line_split[2]\n            label_class = [SemData.class_names['vision24'].index(str(label_class))]\n            \n\n        \n            \n\n\n        # if 0 in label_class:\n        #     label_class.remove(0)\n        # if 255 in label_class:\n        #     label_class.remove(255)\n\n        # print('-----------------')\n        # print(label_class)\n        # print('---------------')\n        \n        #replace each value 255 with 1\n        \n        \n        \n        new_label_class = []\n        raw_label_list = []\n        if(dataset_name == \"vision24\"):  \n            pass\n            # sub_list = [255 if x == 1 else x for x  in sub_list]\n            \n        \n        \n        for c in label_class:\n            \n            if c in sub_list:\n                # c = 255 if c == 1 else c\n                raw_label_list.append(c)\n                # check the area of the mask\n                tmp_label = np.zeros_like(label)\n                \n                target_pix = np.where(label == c)\n               \n                tmp_label[target_pix[0], target_pix[1]] = 1\n                if tmp_label.sum() >= 2 * 32 * 32:\n                    new_label_class.append(c)\n\n\n        label_class = new_label_class\n        \n        # print('label class')\n        # print('--------------------')\n        # print(label_class)\n        # print('--------------------')\n            \n            \n        if(dataset_name == \"vision24\"):  \n            #read the label from the file\n            label_class = line_split[2]\n            label_class = [SemData.class_names['vision24'].index(str(label_class))]\n            sub_list = [1 if x == 255 else x for x  in sub_list]\n            raw_label_list = [1 if x ==255 else x for x in raw_label_list]\n            label_class = [1 if x == 255 else x for x in label_class]\n        \n        \n        item = (\n            image_name.relative_to(data_dir),\n            label_name.relative_to(data_dir),\n            raw_label_list\n        )\n        if len(label_class) > 0:\n            image_label_list.append(item)\n            if(dataset_name != \"vision24\"):\n                #if not vision24 \n                for c in label_class:\n                    if c in sub_list:\n                        sub_class_file_list[c].append(item)\n            else:\n                for c in label_class:\n                    sub_class_file_list[c].append(item)\n    \n    with filepath.open(\"wb\") as f:\n        pickle.dump((image_label_list, sub_class_file_list), f)\n    # print('----------')\n    # print(image_label_list)\n    # print(sub_class_file_list)\n    # print('-----------------')\n    print(f\"Checking image&label list done! There are {len(image_label_list)} images in split {split}.\")\n    # print('sub_class_file_list')\n    # print(sub_class_file_list.keys())\n    \n    return image_label_list, sub_class_file_list\n\n\nclass SemData(Dataset):\n    class_names = {\n        \"pascal\": [\"background\",\n                   \"airplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\",\n                   \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n                   \"dining table\", \"dog\", \"horse\", \"motorbike\", \"person\",\n                   \"potted plant\", \"sheep\", \"sofa\", \"train\", \"tv\"\n                   ],\n        \"coco\": [\"background\",\n                 \"person\", \"airplane\", \"boat\", \"parking meter\", \"dog\", \"elephant\", \"backpack\",\n                 \"suitcase\", \"sports ball\", \"skateboard\", \"wine glass\", \"spoon\", \"sandwich\", \"hot dog\",\n                 \"chair\", \"dining table\", \"mouse\", \"microwave\", \"refrigerator\", \"scissors\",\n                 \"bicycle\", \"bus\", \"traffic light\", \"bench\", \"horse\", \"bear\", \"umbrella\",\n                 \"frisbee\", \"kite\", \"surfboard\", \"cup\", \"bowl\", \"orange\", \"pizza\",\n                 \"couch\", \"toilet\", \"remote\", \"oven\", \"book\", \"teddy bear\",\n                 \"car\", \"train\", \"fire hydrant\", \"bird\", \"sheep\", \"zebra\", \"handbag\",\n                 \"skis\", \"baseball bat\", \"tennis racket\", \"fork\", \"banana\", \"broccoli\", \"donut\",\n                 \"potted plant\", \"tv\", \"keyboard\", \"toaster\", \"clock\", \"hair drier\",\n                 \"motorcycle\", \"truck\", \"stop sign\", \"cat\", \"cow\", \"giraffe\", \"tie\",\n                 \"snowboard\", \"baseball glove\", \"bottle\", \"knife\", \"apple\", \"carrot\", \"cake\",\n                 \"bed\", \"laptop\", \"cell phone\", \"sink\", \"vase\", \"toothbrush\",\n                 ],\n        # \"vision24\":[\"defect\" , \"non-defect\"],\n        \"vision24\" : [\"Cable_thunderbolt\" , \"Cable_torn_apart\" , \"Cylinder_Chip\" , \"Cylinder_Porosity\" ,\"Cylinder_RCS\",\n                          \"PCB_mouse_bite\" , \"PCB_open_circuit\" , \"PCB_short\" , \"PCB_spur\" , \"PCB_spurious_copper\" , \"Screw_front\",\n                          \"Wood_impurities\"]\n \n        \n    }\n\n    def __init__(self, opt, split, shot, query,\n                 data_root=None, data_list=None, transform=None, mode='train',\n                 cache=True):\n        assert mode in ['train', 'val', 'test', 'eval_online']\n\n        if mode != \"train\" and opt.dataset in [\"PASCAL\", \"COCO\" , \"VISION24\"]:\n            mode = \"val\"\n        self.opt = opt\n        self.mode = mode\n        self.split = split\n        self.shot = shot\n        self.query = query\n        self.data_root = Path(data_root)\n        self.tasks = []\n        self.transform = transform\n        self.cache = cache\n\n        if opt.dataset == \"PASCAL\":\n            n_class = 20\n            interval = 5\n            self.class_list = list(range(1, 21))\n            self.sub_val_list = list(range(interval * split + 1, interval * (split + 1) + 1))\n            self.sub_list = list(set(range(1, n_class + 1)) - set(self.sub_val_list))\n            \n            if opt.coco2pascal:\n                n_class = 20\n                self.class_list = list(range(1, 21))\n                if split == 0:\n                    self.sub_val_list = [1, 4, 9, 11, 12, 15]\n                elif split == 1:\n                    self.sub_val_list = [2, 6, 13, 18]\n                elif split == 2:\n                    self.sub_val_list = [3, 7, 16, 17, 19, 20]\n                elif split == 3:\n                    self.sub_val_list = [5, 8, 10, 14]\n                self.sub_list = list(set(range(1, 21)) - set(self.sub_val_list))\n\n        elif opt.dataset == \"COCO\":\n            n_class = 80\n            if opt.use_split_coco:\n                print('INFO: using SPLIT COCO')\n                self.class_list = list(range(1, 81))\n                self.sub_val_list = list(range(split + 1, 81, 4))\n                self.sub_list = list(set(self.class_list) - set(self.sub_val_list))\n            else:\n                interval = 20\n                print('INFO: using COCO')\n                self.class_list = list(range(1, 81))\n                self.sub_val_list = list(range(interval * split + 1, interval * (split + 1) + 1))\n                self.sub_list = list(set(range(1, n_class + 1)) - set(self.sub_val_list))\n        elif opt.dataset == \"VISION24\":\n            n_class =12;\n            interval = 2\n            # self.class_list =[ i for i in range(len(self.class_names['vision24']))] \n            # self.sub_val_list =  [ i for i in range(len(self.class_names['vision24']))]\n            # self.sub_list = [ i for i in range(len(self.class_names['vision24']))]\n\n            self.class_list = [i for i in range(n_class)]\n            self.sub_val_list = [i for i in range(n_class)]\n            self.sub_list = [i for i in range(n_class)]\n            \n            # print('1--------------------------------')\n            # print(self.sub_val_list)\n            # print('\\n\\n')\n            # print('2--------------------------------')\n            # print(self.sub_list)\n        else:\n            raise ValueError(f'Not supported dataset: {opt.dataset}. [PASCAL|COCO | VISION24]')\n\n        self.n_class = n_class\n        self.train_num_classes = len(self.sub_list)\n        if self.mode == 'train':\n            self.data_list, self.sub_cls_files = make_dataset(\n                split, self.data_root, data_list, self.sub_list)\n            \n        else:\n            self.data_list, self.sub_cls_files = make_dataset(\n                split, self.data_root, data_list, self.sub_val_list, opt.coco2pascal)\n\n        # print('----------------------------------')\n        # print(self.data_list)\n        # print(type(self.data_list))\n        # print('-------------------------')\n        for c in self.sub_cls_files:\n            print(f\"Class {c}: {len(self.sub_cls_files[c])} images\")\n\n        self.length_data_list = len(self.data_list)\n        print(f\"Data list length: {self.length_data_list}\")\n        assert self.length_data_list > 0, 'Length of data list is 0. Please make sure the data root ' \\\n                                          f'({self.data_root}) and data list ({data_list}) are correct.'\n        self.length_sub_class_list = {k: len(v) for k, v in self.sub_cls_files.items()}\n\n        self.reset_sampler()\n\n    def __len__(self):\n        if self.mode == 'train':\n            return self.opt.train_n or self.length_data_list\n        else:\n            return self.opt.test_n or self.length_data_list\n\n    def reset_sampler(self):\n        seed = self.opt.seed\n        test_seed = self.opt.test_seed\n        # Use fixed test sampler(opt.test_seed) for reproducibility\n        self.sampler = np.random.RandomState(seed) \\\n            if self.mode == \"train\" else np.random.RandomState(test_seed)\n\n    def sample_tasks(self):\n        self.tasks = []\n\n        total_length = len(self)\n        rounds = (total_length + self.length_data_list - 1) // self.length_data_list\n        counter = 0\n        \n        for r in range(rounds):\n            # Sampling random episodes. Use self.reset_sampler() for fixed sampling orders\n            rng = self.sampler.permutation(np.arange(self.length_data_list)) \n            for idx in rng:\n                item = self.data_list[idx]\n                \n                assert len(item[2]) > 0\n\n                cls = self.sampler.choice(item[2])\n                \n                num_files = len(self.sub_cls_files[cls])\n                \n               \n                s_indices = []\n                for i in range(self.shot):\n                    while True:\n                        # print(f\"Class {cls}: {self.sub_cls_files.get(cls, 'Not Found')}\")\n                        \n\n                        s_idx = self.sampler.choice(num_files, size=1)[0]\n                        \n                        if self.sub_cls_files[cls][s_idx] == item or s_idx in s_indices:\n                            continue\n                        s_indices.append(s_idx)\n                        break\n                self.tasks.append((idx, cls, s_indices))\n            \n            \n                counter += 1\n                if counter >= total_length:\n                    break\n       \n            \n            \n\n    def seg_encode(self, lab, cls, ignore_lab):\n        if self.opt.proc == 'pil':\n            lab = np.array(lab, np.uint8)\n        target_pix = np.where(lab == cls)\n        ignore_pix = np.where(lab == 255)\n        lab[:, :] = 0\n        if target_pix[0].shape[0] > 0:\n            lab[target_pix[0], target_pix[1]] = 1\n        if ignore_pix[0].shape[0] > 0:\n            lab[ignore_pix[0], ignore_pix[1]] = ignore_lab\n        if self.opt.proc == 'pil':\n            lab = Image.fromarray(lab)\n        return lab\n    # def seg_encode(self, lab, cls, ignore_lab):\n    #     if self.opt.proc == 'pil':\n    #         lab = np.array(lab, np.uint8)\n    \n    #     target_pix = np.where(lab == 255)  # Find all pixels with 255\n    #     background_pix = np.where(lab == 0)\n    \n    #     lab[:, :] = 0  # Set all pixels to background (0)\n    #     if target_pix[0].shape[0] > 0:\n    #         lab[target_pix[0], target_pix[1]] = 1  # Convert 255 → 1 for class index\n    #     if ignore_pix[0].shape[0] > 0:\n    #         lab[ignore_pix[0], ignore_pix[1]] = ignore_lab\n    \n    #     if self.opt.proc == 'pil':\n    #         lab = Image.fromarray(lab)\n    \n    #     return lab\n\n\n    def get_image(self, name, cache=True):\n        if self.cache and cache:\n            if name not in cache_image:\n                cache_image[name] = load_image(data_dir / name, 'img', self.opt.proc)\n            return cache_image[name].copy()\n        else:\n            return load_image(data_dir / name, 'img', self.opt.proc)\n\n    def get_label(self, name, cls, ignore_lab=0, cache=True):\n        if self.cache and cache:\n            if name not in cache_label:\n                cache_label[name] = load_image(data_dir / name, 'lab', self.opt.proc)\n            lab = cache_label[name].copy()\n        else:\n            lab = load_image(data_dir / name, 'lab', self.opt.proc)\n        lab = self.seg_encode(lab, cls, ignore_lab)\n        # Convert 255 to 1\n        \n        lab = np.where(lab == 255, 1, 0).astype(np.uint8)\n        return lab\n\n    def get_weights(self, name, cls, cache=True):\n        if self.cache and cache:\n            if name not in cache_weights:\n                cache_weights[name] = load_weights(data_dir / name)\n            weights_dict = cache_weights[name]\n        else:\n            weights_dict = load_weights(data_dir / name)\n        if cls in weights_dict['c']:\n            class_index = weights_dict['c'].index(cls)\n            weights = weights_dict['x'][class_index].copy().astype(np.float32)\n        else:\n            weights = np.zeros(weights_dict['x'][0].shape, np.float32)\n        return weights\n    \n    # def get_weights(self, name, cls, cache=True):\n    #     weight_path = data_dir / name\n    \n    #     # If file doesn't exist, return default zero weights\n    #     if not os.path.exists(weight_path):\n    #         print(f\"Warning: Weight file {weight_path} not found. Using zero weights.\")\n    #         return np.zeros((1, 768), np.float32)  # Adjust shape as needed\n    \n    #     if self.cache and cache:\n    #         if name not in cache_weights:\n    #             cache_weights[name] = load_weights(weight_path)\n    #         weights_dict = cache_weights[name]\n    #     else:\n    #         weights_dict = load_weights(weight_path)\n    \n    #     if cls in weights_dict['c']:\n    #         class_index = weights_dict['c'].index(cls)\n    #         weights = weights_dict['x'][class_index].copy().astype(np.float32)\n    #     else:\n    #         weights = np.zeros(weights_dict['x'][0].shape, np.float32)\n    \n    #     return weights\n\n    def __getitem__(self, index):\n        qry_idx, cls, support_indices = self.tasks[index]\n        image_path, label_path, _ = self.data_list[qry_idx]\n        sup_image_paths = []\n        sup_label_paths = []\n       \n        for sup_idx in support_indices:\n            x_path, y_path, _ = self.sub_cls_files[cls][sup_idx]\n            sup_image_paths.append(x_path)\n            sup_label_paths.append(y_path)\n\n        \n        qry_names = [image_path.stem]\n        sup_names = [x.stem for x in sup_image_paths]\n\n        image = self.get_image(image_path)\n        # keep query ignore_label as 255, which has significant to iou.\n        label = self.get_label(label_path, cls, ignore_lab=255)\n        kwargs = {}\n        sup_kwargs = [{} for _ in range(self.opt.shot)]\n        # Load weights\n        if self.mode == 'train':\n            if self.opt.precompute_weight and self.opt.loss == 'cedt':\n                # Load precomputed weights, used for computing loss\n                weight_path = image_path.parents[1] / f\"weights/{image_path.stem}.npz\"\n                kwargs['weights'] = self.get_weights(weight_path, cls)\n\n\n        \n        sup_images = [self.get_image(x) for x in sup_image_paths]\n        sup_labels = [self.get_label(x, cls, ignore_lab=255) for x in sup_label_paths]\n\n        # raw_label = label.copy()\n        if self.transform is not None:\n            image, label, kwargs = self.transform[1](image, label, **kwargs)\n            for k in range(self.shot):\n                sup_images[k], sup_labels[k], sup_kwargs[k] = self.transform[0](sup_images[k], sup_labels[k],\n                                                                                **sup_kwargs[k])\n\n        sup_images = torch.stack(sup_images, dim=0)\n        sup_labels = torch.stack(sup_labels, dim=0)\n        label = label.unsqueeze(dim=0)\n\n        ret_dict = {\n            'sup_rgb': sup_images,  # [S, 3, H, W]\n            'sup_msk': sup_labels,  # [S, H, W]\n            'qry_rgb': image,  # [3, H, W]\n            'qry_msk': label,  # [1, H, W]\n            'cls': cls,  # [], values in [1, 20] for PASCAL\n            'weights': kwargs.get('weights', None),\n            'sup_names': sup_names,\n            'qry_names': qry_names,\n        }\n        ret_dict = {k: v for k, v in ret_dict.items() if v is not None}\n        return ret_dict\n\n    def get_class_name(self, cls, dataset):\n        return self.class_names[dataset.lower()][cls]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:53.839689Z","iopub.execute_input":"2025-05-06T21:57:53.839978Z","iopub.status.idle":"2025-05-06T21:57:53.853041Z","shell.execute_reply.started":"2025-05-06T21:57:53.839949Z","shell.execute_reply":"2025-05-06T21:57:53.852396Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/data_kits/voc_coco.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/data_kits/datasets.py\n# %load /kaggle/input/fptrans_vision24/pytorch/default/1/FPTrans/data_kits/datasets.py\nimport cv2\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom constants import data_dir, lists_dir\nfrom data_kits import transformation as tf\nfrom data_kits import voc_coco as pfe\nfrom utils_.misc import load_image\nfrom data_kits.binary_dataset import BinarySegmentationDataset\n\n\nDATA_DIR = {\n    \"PASCAL\": data_dir / \"VOCdevkit/VOC2012\",\n    \"COCO\": data_dir / \"COCO\",\n    \"VISION24\": data_dir / \"VISION24\",\n     \n}\nDATA_LIST = {\n    \"PASCAL\": {\n        \"train\": lists_dir / \"pascal/voc_sbd_merge_noduplicate.txt\",\n        \"test\": lists_dir / \"pascal/val.txt\",\n        \"eval_online\": lists_dir / \"pascal/val.txt\"\n    },\n    \"COCO\": {\n        \"train\": lists_dir / \"coco/train_data_list.txt\",\n        \"test\": lists_dir / \"coco/val_data_list.txt\",\n        \"eval_online\": lists_dir / \"coco/val_data_list.txt\"\n    },\n    \"VISION24\": {\n        \"train\": lists_dir / \"vision24/train.txt\",\n        \"test\": lists_dir / \"vision24/test.txt\",\n        \"eval_online\": lists_dir / \"vision24/test.txt\"\n    },\n    \n}\n\n\nMEAN = [0.485, 0.456, 0.406]    # list, normalization mean in data preprocessing\nSTD = [0.229, 0.224, 0.225]     # list, normalization std in data preprocessing\n\n\ndef get_train_transforms(opt, height, width):\n    supp_transform = tf.Compose([tf.RandomResize(opt.scale_min, opt.scale_max),\n                                 tf.RandomRotate(opt.rotate, pad_type=opt.pad_type),\n                                 tf.RandomGaussianBlur(),\n                                 tf.RandomHorizontallyFlip(),\n                                 tf.RandomCrop(height, width, check=True, center=True, pad_type=opt.pad_type),\n                                 tf.ToTensor(mask_dtype='float'),   # support mask using float\n                                 tf.Normalize(MEAN, STD)], processer=opt.proc)\n\n    query_transform = tf.Compose([tf.RandomResize(opt.scale_min, opt.scale_max),\n                                  tf.RandomRotate(opt.rotate, pad_type=opt.pad_type),\n                                  tf.RandomGaussianBlur(),\n                                  tf.RandomHorizontallyFlip(),\n                                  tf.RandomCrop(height, width, check=True, center=True, pad_type=opt.pad_type),\n                                  tf.ToTensor(mask_dtype='long'),   # query mask using long\n                                  tf.Normalize(MEAN, STD)], processer=opt.proc)\n\n    return supp_transform, query_transform\n\n\ndef get_val_transforms(opt, height, width):\n    supp_transform = tf.Compose([tf.Resize(height, width),\n                                 tf.ToTensor(mask_dtype='float'),   # support mask using float\n                                 tf.Normalize(MEAN, STD)], processer=opt.proc)\n\n    query_transform = tf.Compose([tf.Resize(height, width, do_mask=False),  # keep mask the original size\n                                  tf.ToTensor(mask_dtype='long'),   # query mask using long\n                                  tf.Normalize(MEAN, STD)], processer=opt.proc)\n\n    return supp_transform, query_transform\n\n\ndef load(opt, logger, mode):\n    split, shot, query = opt.split, opt.shot, 1\n    height, width = opt.height, opt.width\n\n    if mode == \"train\":\n        data_transform = get_train_transforms(opt, height, width)\n    elif mode in [\"test\", \"eval_online\", \"predict\"]:\n        data_transform = get_val_transforms(opt, height, width)\n    else:\n        raise ValueError(f'Not supported mode: {mode}. [train|eval_online|test|predict]')\n\n    if opt.dataset == \"PASCAL\":\n        num_classes = 20\n        cache = True\n    elif opt.dataset == \"COCO\":\n        num_classes = 80\n        cache = False\n    elif opt.dataset == \"VISION24\":\n        num_classes = 12\n        cache = False\n    else:\n        raise ValueError(f'Not supported dataset: {opt.dataset}. [PASCAL|COCO | VISION24]')\n\n    dataset = pfe.SemData(opt, split, shot, query,\n                          data_root=DATA_DIR[opt.dataset],\n                          data_list=DATA_LIST[opt.dataset][mode],\n                          transform=data_transform,\n                          mode=mode,\n                          cache=cache)\n\n    dataloader = DataLoader(dataset,\n                            batch_size=opt.bs if mode == 'train' else opt.test_bs,\n                            shuffle=True if mode == 'train' else False,\n                            num_workers=opt.num_workers,\n                            pin_memory=True,\n                            drop_last=True if mode == 'train' else False )\n\n    logger.info(' ' * 5 + f\"==> Data loader {opt.dataset} for {mode}\")\n    return dataset, dataloader, num_classes\n\n\ndef get_val_labels(opt, mode):\n    if opt.dataset == \"PASCAL\":\n        if opt.coco2pascal:\n            if opt.split == 0:\n                sub_val_list = [1, 4, 9, 11, 12, 15]\n            elif opt.split == 1:\n                sub_val_list = [2, 6, 13, 18]\n            elif opt.split == 2:\n                sub_val_list = [3, 7, 16, 17, 19, 20]\n            elif opt.split == 3:\n                sub_val_list = [5, 8, 10, 14]\n            else:\n                raise ValueError(f'PASCAL only have 4 splits [0|1|2|3], got {opt.split}')\n        else:\n            sub_val_list = list(range(opt.split * 5 + 1, opt.split * 5 + 6))\n        return sub_val_list\n    elif opt.dataset == \"COCO\":\n        if opt.use_split_coco:\n            return list(range(opt.split + 1, 81, 4))\n        return list(range(opt.split * 20 + 1, opt.split * 20 + 21))\n    elif opt.dataset == \"VISION24\":\n        # return list(range(opt.split + 1, 6))\n        return [0 , 1 , 2 , 3  ,4 , 5]\n    else:\n        raise ValueError(f'Only support datasets [PASCAL|COCO|VISION24], got {opt.dataset}')\n\n\ndef load_p(opt, device):\n    supp_t, query_t = get_val_transforms(opt, opt.height, opt.width)\n    p = opt.p\n\n    if p.sup and p.qry:\n        supp_rgb_path = DATA_DIR[opt.dataset] / \"JPEGImages\" / f\"{p.sup}.jpg\"\n        supp_lab_path = DATA_DIR[opt.dataset] / \"SegmentationClassAug\" / f\"{p.sup}.png\"\n        query_rgb_path = DATA_DIR[opt.dataset] / \"JPEGImages\" / f\"{p.qry}.jpg\"\n        query_lab_path = DATA_DIR[opt.dataset] / \"SegmentationClassAug\" / f\"{p.qry}.png\"\n\n        supp_rgb = load_image(supp_rgb_path, 'img', opt.proc)\n        _supp_lab = load_image(supp_lab_path, 'lab', opt.proc)\n        supp_lab = np.zeros_like(_supp_lab, dtype=_supp_lab.dtype)\n        supp_lab[_supp_lab == 255] = 255\n        supp_lab[_supp_lab == p.cls] = 1\n        query_ori = query_rgb = load_image(query_rgb_path, 'img', opt.proc)\n        query_lab = np.zeros(query_rgb.shape[:-1], dtype=_supp_lab.dtype)\n        _query_lab = load_image(query_lab_path, 'lab', opt.proc)\n        query_lab[_query_lab == 255] = 255\n        query_lab[_query_lab == p.cls] = 1\n\n        supp_img, supp_lab, _ = supp_t(supp_rgb, supp_lab)\n        query_img, query_lab, _ = query_t(query_rgb, query_lab)\n\n        supp_img = supp_img[None, None].to(device)      # [B, S, 3, H, W]\n        supp_lab = supp_lab[None, None].to(device)      # [B, S, H, W]\n        query_img = query_img[None].to(device)          # [B, 3, H, W]\n        query_lab = query_lab[None].to(device)      # [B, H, W]\n    elif p.sup_rgb and p.sup_msk and p.qry_rgb:\n        _supp_rgbs = [load_image(x, 'img', opt.proc) for x in p.sup_rgb]\n        _supp_labs = [load_image(x, 'lab', opt.proc, mode=cv2.IMREAD_UNCHANGED) for x in p.sup_msk]\n        supp_rgbs = []\n        supp_labs = []\n        for i, _supp_lab in enumerate(_supp_labs):\n            if len(_supp_lab.shape) != 2:\n                _supp_lab = _supp_lab[:, :, -1]\n            supp_lab = np.zeros_like(_supp_lab, dtype=_supp_lab.dtype)\n            if p.cls == 255:\n                supp_lab[_supp_lab == p.cls] = 1\n            else:\n                supp_lab[_supp_lab == 255] = 255\n                supp_lab[_supp_lab == p.cls] = 1\n            supp_img, supp_lab, _ = supp_t(_supp_rgbs[i], supp_lab)\n            supp_rgbs.append(supp_img)\n            supp_labs.append(supp_lab)\n        supp_img = torch.stack(supp_rgbs, dim=0)\n        supp_lab = torch.stack(supp_labs, dim=0)\n\n        query_ori = [load_image(x, 'img', opt.proc) for x in p.qry_rgb]\n        _query_rgbs = query_ori\n        _query_labs = [np.zeros(x.shape[:-1], dtype=_supp_labs[0].dtype) for x in _query_rgbs]\n        query_rgbs = []\n        for i, _query_lab in enumerate(_query_labs):\n            query_img, query_lab, _ = query_t(_query_rgbs[i], _query_lab)\n            query_rgbs.append(query_img)\n        query_img = torch.stack(query_rgbs, dim=0)\n\n        supp_img = supp_img[None].to(device)            # [B, S, 3, H, W]\n        supp_lab = supp_lab[None].to(device)            # [B, S, H, W]\n        query_img = query_img.to(device)                # [Q, 3, H, W]\n        query_lab = None\n    else:\n        raise ValueError(f'In the prediction mode, either the [p.sup, p.qry] or the \\n'\n                         f'[p.sup_rgb, p.sup_msk, p.qry_rgb] should be given. Got \\n'\n                         f'    p.sup={p.sup}, p.qry={p.qry}\\n'\n                         f'    p.sup_rgb={p.sup_rgb}, p.sup_msk={p.sup_msk}, p.qry_rgb={p.qry_rgb}')\n\n\n    return supp_img, supp_lab, query_img, query_lab, query_ori\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:53.956893Z","iopub.execute_input":"2025-05-06T21:57:53.957596Z","iopub.status.idle":"2025-05-06T21:57:53.965309Z","shell.execute_reply.started":"2025-05-06T21:57:53.957566Z","shell.execute_reply":"2025-05-06T21:57:53.964540Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/data_kits/datasets.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%cd /kaggle/working/FPTrans","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:53.966584Z","iopub.execute_input":"2025-05-06T21:57:53.966822Z","iopub.status.idle":"2025-05-06T21:57:53.982677Z","shell.execute_reply.started":"2025-05-06T21:57:53.966799Z","shell.execute_reply":"2025-05-06T21:57:53.982154Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/FPTrans\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/core/base_trainer.py\n# %load /kaggle/working/FPTrans/core/base_trainer.py\nimport os\nimport random\nimport shutil\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom tqdm import tqdm\n\nfrom constants import on_cloud\nfrom core import losses as loss_utils\nfrom core import solver\nfrom core.metrics import FewShotMetric, Accumulator\nfrom data_kits import datasets\nfrom utils_.loggers import C as CC\nfrom utils_.timer import Timer\n\n\ndef round_(array):\n    if isinstance(array, float) or array.ndim == 0:\n        return f\"{array:5.2f}\"\n    if array.ndim == 1:\n        return \"[\" + \", \".join([f\"{x:5.2f}\" for x in array]) + \"]\"\n\n\ndef save_img(file_path, img):\n    pil_img = Image.fromarray(img.astype(np.uint8))\n    # pil_img.putpalette(palette)\n    pil_img.save(file_path)\n\n\ndef pad(str0, padded, length, align='left'):\n    remains = length - len(str0)\n    if align == 'left':\n        left = 1\n    elif align == 'center':\n        left = remains // 2\n    else:\n        raise ValueError\n    right = remains - left\n    return padded * left + str0 + padded * right\n\n\nclass BaseEvaluator(object):\n    \"\"\"\n    Evaluator base class. Evaluator is used in the validation stage and testing stage.\n    All the evaluators should inherit from this class and implement the `test_step()`\n    function.\n\n    Parameters\n    ----------\n    opt: misc.MapConfig\n        Experiment configuration.\n    model: nn.Module\n        PyTorch model instance.\n    mode: str\n        Evaluation mode. [EVAL_ONLINE, EVAL]\n\n    \"\"\"\n    def __init__(self, opt, logger, device, model, model_T, mode, need_grad=False):\n        self.opt = opt\n        self.logger = logger\n        self.device = device\n        self.mode = mode\n        if mode not in [\"EVAL_ONLINE\", \"EVAL\"]:\n            raise ValueError(f\"Not supported evaluation mode {mode}. [EVAL_ONLINE, EVAL]\")\n        self.num_devices = torch.cuda.device_count()\n\n        self.model = model\n        if not isinstance(self.model, nn.DataParallel):\n            self.model_DP = self.init_device(self.model)\n        else:\n            self.model_DP = self.model\n        # if model_T is not None:\n        #     self.model_T = model_T\n        #     if not isinstance(self.model_T, nn.DataParallel):\n        #         self.model_T_DP = self.init_device(self.model_T)\n        #     else:\n        #         self.model_T_DP = self.model_T\n        # else:\n        #     self.model_T = None\n        #     self.model_T_DP = None\n        \n        self.loss_obj = loss_utils.get(opt, logger, loss=opt.loss.replace('dt', ''))\n        self.need_grad = need_grad\n\n    def test_step(self, batch, step):\n        raise NotImplementedError\n\n    def init_device(self, net):\n        net = net.to(self.device)\n        net_DP = nn.DataParallel(net, device_ids=range(self.num_devices))\n        return net_DP\n\n    def maybe_print_metrics(self, accu, gen, epoch=None):\n        if self.opt.tqdm:\n            if epoch is not None:\n                print_str = f\"[{self.mode}] [{epoch}/{self.opt.epochs}] loss: {accu.mean('loss'):.5f}\"\n            else:\n                print_str = f\"[{self.mode}] loss: {accu.mean('loss'):.5f}\"\n            gen.set_description(print_str)\n\n    @torch.no_grad()\n    def start_eval_loop(self, data_loader, num_classes, epoch=None):\n        \"\"\" For few-shot learning \"\"\"\n        # Set model to evaluation mode (for specific layers, such as batchnorm, dropout, dropblock)\n        self.model.eval()\n        # Fix sampling order of the test set.\n        data_loader.dataset.reset_sampler()\n        timer = Timer()\n        data_timer = Timer()\n        val_labels = datasets.get_val_labels(self.opt, self.mode)\n        # create saving directory\n        save_dir = None\n        if self.opt.save_dir is not None:\n            save_dir = Path(self.opt.save_dir)\n            save_dir.mkdir(parents=True, exist_ok=True)\n\n        fs_metric = FewShotMetric(num_classes)    \n        accu = Accumulator(loss=0)\n        data_loader.dataset.sample_tasks()\n\n        gen = data_loader\n        if self.opt.tqdm:\n            gen = tqdm(gen, leave=True)\n        data_timer.tic()\n        for i, batch in enumerate(gen, start=1):\n            data_timer.toc()\n            with timer.start():\n                qry_pred, losses = self.test_step(batch, i)\n                accu.update(**losses)\n            self.maybe_print_metrics(accu, gen, epoch)\n            fs_metric.update(qry_pred, batch['qry_msk'], batch['cls'],verbose =0 )\n\n            # save prediction\n            if save_dir:\n                qry_mask = batch['qry_msk'].cpu().numpy()[:, 0]\n                qry_pred = qry_pred[:, 0]\n                classes = batch['cls'].cpu().numpy().tolist()\n                for j, qry_name in enumerate(batch['qry_names'][0]):\n                    if batch['qry_ori_size'] is not None:\n                        ori_H, ori_W = batch['qry_ori_size'][j]\n                        p = qry_pred[j, :ori_H, :ori_W] * 255\n                        r = (qry_mask[j, :ori_H, :ori_W] == 1) * 255\n                        save_path = save_dir / (f'{(i - 1) * self.opt.test_bs + j + 1:04d}_' + qry_name + '_pred.png')\n                        save_img(save_path, p)\n                        save_path = save_dir / (f'{classes[j]:02d}_' + qry_name + '_mask.png')\n                        save_img(save_path, r)\n            data_timer.tic()\n\n        miou_class, miou_avg , catch_rate , yeild_rate = fs_metric.get_scores(val_labels)\n        biou_class, biou_avg , _ , _ = fs_metric.get_scores(val_labels, binary=True)\n        str1 = f'mIoU mean: {round_(miou_class * 100)} ==> {round_(miou_avg * 100)}'\n        str2 = f'bIoU mean: {round_(biou_class * 100)} ==> {round_(biou_avg * 100)}'\n\n        if self.mode == \"EVAL_ONLINE\":\n            self.logger.info(str1)\n            self.logger.info(str2)\n        elif self.mode == \"EVAL\":\n            str3 = f'speed: {round_(timer.cps)} FPS'\n            max_length = max(len(str1), len(str2), len(str3)) + 2\n            self.logger.info('╒' + pad(' Final Results ', '═', max_length, align='center') + '╕')\n            self.logger.info('│' + pad(str1, ' ', max_length) + '│')\n            self.logger.info('│' + pad(str2, ' ', max_length) + '│')\n            self.logger.info('│' + pad(str3, ' ', max_length) + '│')\n            self.logger.info('╘' + pad('', '═', max_length) + '╛')\n\n        return accu.mean('loss'), miou_avg, biou_avg, timer.elapsed, data_timer.elapsed , catch_rate, yeild_rate\n\n\nclass BaseTrainer(object):\n    def __init__(self, opt, logger, device, model, data_loader, data_loader_val, _run):\n        self.opt = opt\n        self.logger = logger\n        self.device = device\n        self.run = _run\n        self.data_loader = data_loader\n        self.data_loader_val = data_loader_val\n        self.num_devices = torch.cuda.device_count()\n\n        # Define model-related objects\n        self.model = model\n        self.model_DP = self.init_device(self.model)\n        self.loss_obj = loss_utils.get(opt, logger)\n\n        self.build_optimizer(verbose=1)\n        self.step_lr_counter = 0\n\n        # Define model_dir for saving checkpoints\n        self.do_ckpt = True\n        self.log_dir = _run.run_dir\n        if on_cloud:\n            self.cloud_save_dir = Path(f\"afs/output/models_fss/{self.log_dir.name}\")\n            if not self.cloud_save_dir.exists():\n                self.cloud_save_dir.mkdir(parents=True)\n\n        # Define metrics and output templates\n        self.best_iou = -1.\n        self.template = \"[{:d}/{:d}]\" + \\\n                        \" | Tr {:6.4f} | Val {:6.4f} | mIoU {:5.2f} | bIoU {:5.2f}\" + \\\n                        \" | DATA {:s} | OPT {:s} | ETA {:s} | Catch Rate {:5.2f} | Yeild Rate {:5.2f}\"\n\n        self.loss_names = 'loss prompt pair'.split()\n        self.steps_per_epoch = len(self.data_loader)\n\n    def build_optimizer(self, verbose=0):\n        opt = self.opt\n        model = self.model\n\n        param_list = model.get_params_list()\n        max_steps = opt.epochs * len(self.data_loader)\n        self.optimizer, self.scheduler = solver.get(opt, param_list, max_steps)\n\n        if verbose:\n            plists = model.get_params_list()\n            for plist in plists:\n                self.logger.info(f\"Number of trainable parameters: {len([_ for _ in plist['params']])}\")\n\n    def train_step(self, batch, step, epoch):\n        raise NotImplementedError\n\n    def init_device(self, net):\n        net = net.to(self.device)\n        net_DP = nn.DataParallel(net, device_ids=range(self.num_devices))\n        return net_DP\n\n    @staticmethod\n    def second2str(seconds):\n        seconds = int(seconds)\n        hours = seconds // 3600\n        minutes = (seconds % 3600) // 60\n        seconds_ = seconds % 60\n        return f\"{hours:02d}:{minutes:02d}:{seconds_:02d}\"\n\n    def time_left(self, iters, epoch, speed, extra_per_epoch=0):\n        iters_left = (self.opt.epochs - epoch) * self.steps_per_epoch + (self.steps_per_epoch - iters)\n        time_left = int(iters_left * speed) + (self.opt.epochs - epoch) * extra_per_epoch\n        return time_left\n\n    def maybe_print_losses(self, iters, epoch, accu, timer, gen, data_timer):\n        if self.opt.tqdm:\n            fmt_str = \"[TRAIN] [{:d}/{:d}] \"\n            for name in self.loss_names:\n                fmt_str += name + \": {:.4f} \"\n            print_str = fmt_str.format(epoch, self.opt.epochs, *accu.mean(self.loss_names))\n            gen.set_description(print_str)\n        elif iters % self.opt.print_interval == 0 or iters == self.steps_per_epoch:\n            fmt_str = \"[{:d}/{:d}] [{:d}/{:d}] \"\n            for name in self.loss_names:\n                fmt_str += name + \": {:.4f} \"\n            fmt_str += \"[SPD: {:.2f}]\"\n            print_str = fmt_str.format(\n                epoch, self.opt.epochs, \n                iters, self.steps_per_epoch, \n                *accu.mean(self.loss_names), \n                timer.spc + data_timer.spc,\n            )\n            self.logger.info(print_str)\n\n    def print_lr(self):\n        for i, group in enumerate(self.optimizer.param_groups):\n            self.logger.info(f\"Learning rate for {self.opt.optim.upper()} param_group[{i}] is {group['lr']:.6g}\")\n\n    def start_training_loop(self, start_epoch, evaluator, num_classes):\n        timer = Timer()\n        data_timer = Timer()\n        accu = Accumulator(**{x: 0. for x in self.loss_names})\n\n        for epoch in range(start_epoch, self.opt.epochs + 1):\n            self.print_lr()\n            \n            # Load previous epoch weights if available\n            # if(self.opt.dataset == \"VISION24\"):\n            #     checkpoint_path = os.path.join(\"vision24\", f\"epoch_{epoch-1}.pth\")\n            #     if os.path.exists(checkpoint_path):\n            #         print(f\"Loading weights from {checkpoint_path}...\")\n            #         checkpoint = torch.load(checkpoint_path)\n            #         self.model.load_state_dict(checkpoint['model_state_dict'])\n            #         self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            #         accu.update(**checkpoint['accu'])  # Restore accumulated loss\n            \n\n            # 1. Training\n            self.model.train()\n            self.data_loader.dataset.sample_tasks()\n            gen = self.data_loader\n            if self.opt.tqdm:\n                gen = tqdm(gen, leave=True)\n\n            for i, batch in enumerate(gen, start=1):\n                if i > 1:\n                    data_timer.toc()\n                with timer.start():\n                    losses = self.train_step(batch, i, epoch)\n                    accu.update(**losses)\n                self.maybe_print_losses(i, epoch, accu, timer, gen, data_timer)\n                self.step_lr()\n                data_timer.tic()\n\n            # 2. Evaluation\n            if self.opt.ckpt_interval > 0 and epoch % self.opt.ckpt_interval == 0 or epoch == self.opt.epochs:\n                mloss, miou, biou, val_elapsed, val_data_elapsed , catch_rate ,yeild_rate = evaluator.start_eval_loop(self.data_loader_val, num_classes, epoch)\n                best = self.snapshot(epoch, miou, biou)\n                \n                data_time = data_timer.elapsed + val_data_elapsed\n                epoch_time = timer.elapsed + val_elapsed\n                self.log_result(epoch, accu, mloss, miou, biou, best, epoch_time, data_time , catch_rate , yeild_rate)\n                \n            # Save model checkpoint after every epoch\n            # if(self.opt.dataset == \"VISION24\"):\n            #     checkpoint_path = os.path.join('vision24', f\"epoch_{epoch}.pth\")\n            #     torch.save({\n            #         'epoch': epoch,\n            #         'model_state_dict': self.model.state_dict(),\n            #         'optimizer_state_dict': self.optimizer.state_dict(),\n            #         'accu': accu.get(),  # Save accumulated loss\n            #     }, checkpoint_path)\n\n            #     print(f\"Checkpoint saved: {checkpoint_path}\")\n\n            # 3. Prepare for next epoch\n            timer.reset()\n            data_timer.reset()\n            accu.reset()\n\n    def step_lr(self):\n        \"\"\"\n        Update learning rate by the specified learning rate policy.\n        For 'cosine' and 'poly' policies, the learning rate is updated by steps.\n        For other policies, the learning rate is updated by epochs.\n        \"\"\"\n        if self.scheduler is None:\n            return\n\n        self.step_lr_counter += 1\n\n        if self.opt.lrp in [\"cosine\", \"poly\", \"cosinev2\"]:   # forward per step\n            self.scheduler.step()\n        elif self.step_lr_counter == self.steps_per_epoch:      # forward per epoch\n            self.scheduler.step()\n            self.step_lr_counter = 0\n\n    def get_weights(self):\n        state_dict = self.model.state_dict()\n        state_dict = {k: v for k, v in state_dict.items() if 'original_encoder' not in k}\n        return state_dict\n\n    def snapshot(self, epoch, miou, biou, verbose=0):\n        best = False\n        if on_cloud and self.log_dir.stem == \"None\":\n            if miou > self.best_iou:\n                best = True\n                self.best_iou = miou\n            return best\n\n        if miou > self.best_iou:\n            best = True\n            self.best_iou = miou\n\n        save_path = self.log_dir / \"ckpt.pth\"\n        state = {\n            'epoch': epoch,\n            'miou': miou,\n            'biou': biou,\n            'best_miou': self.best_iou,\n            'model_state': self.get_weights(),\n        }\n\n        torch.save(state, save_path)\n        if verbose:\n            self.logger.info(CC.c(f\" \\\\_/ Save checkpoint to {save_path}\", CC.OKGREEN))\n\n        if best:\n            shutil.copyfile(save_path, self.log_dir / \"bestckpt.pth\")\n        \n        # Make a copy when oncloud\n        if on_cloud:\n            state.update({\n                'random_state': random.getstate(),\n                'np_random_state': np.random.get_state(),\n                'torch_random_state': torch.get_rng_state().numpy(),\n                'train_sampler_state': self.data_loader.dataset.sampler.get_state(),\n                'val_sampler_state': self.data_loader_val.dataset.sampler.get_state(),\n                'optimizer_state': self.optimizer.state_dict(),\n                'scheduler_state': self.scheduler.state_dict(),\n            })\n\n            try:\n                on_cloud_save_path = self.cloud_save_dir / \"ckpt.pth\"\n                if on_cloud_save_path.exists():\n                    os.remove(on_cloud_save_path)\n                torch.save(state, on_cloud_save_path)\n            except Exception as e:\n                print(e)\n            \n            if best:\n                try:\n                    on_cloud_save_path_best = self.cloud_save_dir / \"bestckpt.pth\"\n                    if on_cloud_save_path_best.exists():\n                        os.remove(on_cloud_save_path_best)\n                    shutil.copyfile(save_path, on_cloud_save_path_best)\n                except Exception as e:\n                    print(e)\n        return best\n\n    def log_result(self, epoch, accu, val_loss, val_mIoU, val_bIoU, best, epoch_time, data_time , catch_rate , yeild_rate, **kwargs):\n        # Log epoch summary to the terminal\n        losses = accu.mean('loss')\n        log_str = self.template.format(\n            epoch, \n            self.opt.epochs, \n            losses, \n            val_loss, \n            val_mIoU * 100, \n            val_bIoU * 100, \n            self.second2str(data_time),\n            self.second2str(epoch_time),\n            self.second2str((self.opt.epochs - epoch) * epoch_time),\n            catch_rate * 100,\n            yeild_rate * 100\n            \n        ) + \" | (best)\" * best\n        self.logger.info(log_str)\n        self.logger.info(CC.c(f\"[{epoch}/{self.opt.epochs}] Best mIoU until now: {self.best_iou * 100:.2f}\\n\", CC.OKGREEN))\n\n        # Log results to the sacred database\n        for k, v in accu.mean(self.loss_names, dic=True).items():\n            self.run.log_scalar(k, float(v), epoch)\n        self.run.log_scalar('val_loss', float(val_loss), epoch)\n        self.run.log_scalar('val_mIoU', float(val_mIoU), epoch)\n        self.run.log_scalar('val_bIoU', float(val_bIoU), epoch)\n        for k, v in kwargs.items():\n            self.run.log_scalar(k, float(v), epoch)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:53.983714Z","iopub.execute_input":"2025-05-06T21:57:53.983985Z","iopub.status.idle":"2025-05-06T21:57:54.001669Z","shell.execute_reply.started":"2025-05-06T21:57:53.983953Z","shell.execute_reply":"2025-05-06T21:57:54.001100Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/core/base_trainer.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/core/metrics.py\n\n# %load /kaggle/working/FPTrans/core/metrics.py\nimport numpy as np\n\n\nclass FewShotMetric(object):\n    def __init__(self, n_class):\n        self.n_class = n_class\n        self.stat = np.zeros((self.n_class + 1, 3))     # +1 for bg, 3 for tp, fp, fn\n\n    def update(self, pred, ref, cls, ori_size=None, verbose=0):\n        pred = np.asarray(pred, np.uint8)\n        ref = np.asarray(ref, np.uint8)\n        for i, ci in enumerate(cls):    # iter on batch ==> [episode_1, episode_2, ...]\n            \n            p = pred[i]\n            r = ref[i]\n            if ori_size is not None:\n                ori_H, ori_W = ori_size[i]\n                p = p[:, :ori_H, :ori_W]\n                r = r[:, :ori_H, :ori_W]\n           \n            # for j, c in enumerate([0, 1 if int(ci) == 255 else int(ci)]):     # iter on class ==> [bg_cls, fg_cls]\n            \n            # for j, c in enumerate([0, int(ci)]):\n            for j, c in enumerate([0, 1]):\n                tp = int((np.logical_and(p == j, r != 255) * np.logical_and(r == j, r != 255)).sum())\n                fp = int((np.logical_and(p == j, r != 255) * np.logical_and(r != j, r != 255)).sum())\n                fn = int((np.logical_and(p != j, r != 255) * np.logical_and(r == j, r != 255)).sum())\n                if verbose:\n                    print(tp / (tp + fp + fn))\n                \n                self.stat[c, 0] += tp\n                self.stat[c, 1] += fp\n                self.stat[c, 2] += fn\n\n    def get_scores(self, labels, binary=False):\n        \"\"\"\n        Parameters\n        ----------\n        labels: list, compute mean iou on which classes\n        binary: bool, return mean iou (average on foreground classes) \n            or binary iou (average on foreground and background)\n\n        Returns\n        -------\n        mIoU_class: np.ndarray, all ious of foreground classes\n        mean: float, mean iou over foreground classes\n        \"\"\"\n        labels = [0 if lbl == 0 else 1 for lbl in labels]\n        if binary:\n            stat = np.c_[self.stat[0], self.stat[1:].sum(axis=0)].T     # [2, 3]\n        else:\n            stat = self.stat[labels]                                    # [N, 3]\n        \n        tp, fp, fn = stat.T                                             # [2 or N]\n        mIoU_class = tp / (tp + fp + fn)                                # [2 or N]\n        mean = np.nanmean(mIoU_class)                                   # scalar\n\n        return mIoU_class, mean\n\n    def reset(self):\n        self.stat = np.zeros((self.n_class + 1, 3))     # +1 for bg, 3 for tp, fp, fn\n\n\nclass SegmentationMetric(object):\n    def __init__(self, n_class):\n        self.n_class = n_class\n        self.confusion_matrix = np.zeros((n_class, n_class))\n\n    def _fast_hist(self, label_true, label_pred, n_class):\n        mask = (label_true >= 0) & (label_true < n_class)\n        hist = np.bincount(\n            n_class * label_true[mask].astype(int) + label_pred[mask],\n            minlength=n_class ** 2,\n        ).reshape(n_class, n_class)\n        return hist\n\n    def update(self, pred, ref, ori_size=None):\n        pred = np.asarray(pred, np.uint8)\n        ref = np.asarray(ref, np.uint8)\n        for i, (r, p) in enumerate(zip(ref, pred)):\n            if ori_size is not None:\n                ori_H, ori_W = ori_size[i]\n                p = p[:ori_H, :ori_W]\n                r = r[:ori_H, :ori_W]\n            self.confusion_matrix += self._fast_hist(\n                r.flatten(), p.flatten(), self.n_class\n            )\n\n    def get_scores(self, binary=True, withbg=True):\n        if not binary:\n            hist = self.confusion_matrix\n        else:\n            hist = np.zeros((2, 2), dtype=np.int32)\n            hist[0, 0] = self.confusion_matrix[0, 0]\n            hist[0, 1] = self.confusion_matrix[0, 1:].sum()\n            hist[1, 0] = self.confusion_matrix[1:, 0].sum()\n            hist[1, 1] = self.confusion_matrix[1:, 1:].sum()\n        iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n        if not withbg:\n            iou = iou[1:]\n        mean_iou = np.nanmean(iou)\n\n        return iou, mean_iou\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((self.n_class, self.n_class))\n\n\nclass Accumulator(object):\n    def __init__(self, **kwargs):\n        self.values = kwargs\n        self.counter = {k: 0 for k, v in kwargs.items()}\n        for k, v in self.values.items():\n            if not isinstance(v, (float, int, list)):\n                raise TypeError(f\"The Accumulator does not support `{type(v)}`. Supported types: [float, int, list]\")\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(self.values[k], list):\n                self.values[k].append(v)\n            else:\n                self.values[k] = self.values[k] + v\n            self.counter[k] += 1\n\n    def reset(self):\n        for k in self.values.keys():\n            if isinstance(self.values[k], list):\n                self.values[k] = []\n            else:\n                self.values[k] = 0\n            self.counter[k] = 0\n\n    def mean(self, key, axis=None, dic=False):\n        if isinstance(key, str):\n            if isinstance(self.values[key], list):\n                return np.array(self.values[key]).mean(axis)\n            else:\n                return self.values[key] / (self.counter[key] + 1e-7)\n        elif isinstance(key, (list, tuple)):\n            if dic:\n                return {k: self.mean(k, axis) for k in key}\n            return [self.mean(k, axis) for k in key]\n        else:\n            TypeError(f\"`key` must be a str/list/tuple, got {type(key)}\")\n\n    def std(self, key, axis=None, dic=False):\n        if isinstance(key, str):\n            if isinstance(self.values[key], list):\n                return np.array(self.values[key]).std(axis)\n            else:\n                raise RuntimeError(\"`std` is not supported for (int, float). Use list instead.\")\n        elif isinstance(key, (list, tuple)):\n            if dic:\n                return {k: self.std(k) for k in key}\n            return [self.std(k) for k in key]\n        else:\n            TypeError(f\"`key` must be a str/list/tuple, got {type(key)}\")\n    def get(self):\n        \"\"\"Returns the current accumulated values.\"\"\"\n        return {k: v.copy() if isinstance(v, list) else v for k, v in self.values.items()}\n    \n    def load(self, data):\n        \"\"\"Loads accumulated values from a dictionary (used for resuming training).\"\"\"\n        for k, v in data.items():\n            if k in self.values:\n                self.values[k] = v.copy() if isinstance(v, list) else v\n                self.counter[k] = len(v) if isinstance(v, list) else 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:54.003124Z","iopub.execute_input":"2025-05-06T21:57:54.003340Z","iopub.status.idle":"2025-05-06T21:57:54.019766Z","shell.execute_reply.started":"2025-05-06T21:57:54.003320Z","shell.execute_reply":"2025-05-06T21:57:54.019114Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/core/metrics.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/core/metrics.py\n#NEW CODE \nimport numpy as np\n\n\nclass FewShotMetric(object):\n    def __init__(self, n_class):\n        # Ignore n_class; track binary IoU (defect vs. background)\n        self.n_class = n_class  # Kept for compatibility\n        self.stat = np.zeros(7)  # [tp, fp, fn , good_catch , positive_pairs , correct_yeild , negative_yeild]\n\n    def update(self, pred, ref, cls, ori_size=None, verbose=0):\n        \"\"\"\n        Update metrics for Vision24 defect segmentation (.binary IoU).\n\n        Args:\n            pred: Predicted masks [batch, H, W] or [batch, 1, H, W]\n            ref: Ground truth masks [batch, H, W], expected to be binary (0, 1)\n            cls: Class indices (ignored)\n            ori_size: Original sizes [(H, W), ...] (optional)\n            verbose: If > 0, print IoU per sample\n        \"\"\"\n        pred = np.asarray(pred, np.uint8)\n        ref = np.asarray(ref, np.uint8)\n\n        # Validate ground truth: must be binary (0, 1)\n        unique_ref = np.unique(ref)\n        if not np.all(np.isin(unique_ref, [0, 1])):\n            raise ValueError(f\"Ground truth masks must be binary (0, 1), got values: {unique_ref}\")\n\n        # Convert predictions to binary (threshold > 0)\n        pred = (pred > 0).astype(np.uint8)\n\n        for i in range(pred.shape[0]):  # Iterate over batch\n            p = pred[i]\n            r = ref[i]\n\n            if ori_size is not None:\n                ori_H, ori_W = ori_size[i]\n                p = p[:ori_H, :ori_W]\n                r = r[:ori_H, :ori_W]\n\n            # Binary IoU calculation\n            tp = np.sum((p == 1) & (r == 1))  # True positives\n            fp = np.sum((p == 1) & (r == 0))  # False positives\n            fn = np.sum((p == 0) & (r == 1))  # False negatives\n\n            #iou\n            iou = tp / (tp + fp + fn + 1e-7)\n\n            #yeild rate\n            if(np.sum((r == 1)) == 0 ):\n                #find the all white value in the predict\n                defect_in_pred = np.sum((p == 1))\n                #all mask area\n                all_mask = np.sum((p == 1) & (p == 0))\n\n                #percentage of white space to all space\n                percentage  = (defect_in_pred) / (all_mask)\n                \n                #Adding for the negative pairs , where the mask with no defect \n                self.stat[6] += 1\n                \n                if(precentage <= 0.000239):\n                    self.stat[5] += 1  # good yeild\n            else:\n                # the number of positive pairs ( where the mask has a defect)\n                self.stat[4] +=1\n                \n                iou = tp / (tp + fp + fn + 1e-7)\n\n                \n                if(iou  >= 0.3):\n                    #number of good catch where the defect was detected with IOU >= 0.3\n                    self.stat[3] +=1 \n            \n            if verbose:\n                #This is a debut if statement\n                iou = tp / (tp + fp + fn + 1e-7)         \n                print(f\"Sample {i} IoU: {iou:.4f}\")\n\n            self.stat[0] += tp\n            self.stat[1] += fp\n            self.stat[2] += fn\n\n    def get_scores(self, labels, binary=False):\n        \"\"\"\n        Compute binary IoU.\n\n        Args:\n            labels: Ignored, kept for compatibility\n            binary: Ignored, always True for Vision24\n\n        Returns:\n            mIoU_class: np.ndarray, [iou] (single IoU value)\n            mean: float, Same as iou (for compatibility)\n        \"\"\"\n        tp, fp, fn ,good_catch,postive_pairs , correct_yeild , negative_pairs = self.stat\n        iou = tp / (tp + fp + fn + 1e-7)\n\n        catch_rate=  (good_catch) / (postive_pairs)\n        yeild_rate = (correct_yeild) / (negative_pairs)\n        \n        return np.array([iou]), iou, catch_rate, yeild_rate\n\n    def reset(self):\n        self.stat = np.zeros(7)\n\n\nclass SegmentationMetric(object):\n    def __init__(self, n_class):\n        # Ignore n_class; use 2x2 confusion matrix for binary IoU\n        self.n_class = n_class  # Kept for compatibility\n        self.confusion_matrix = np.zeros((2, 2))  # [background, foreground]\n\n    def _fast_hist(self, label_true, label_pred):\n        # Binary histogram (0, 1)\n        mask = (label_true >= 0) & (label_true <= 1)\n        hist = np.bincount(\n            2 * label_true[mask].astype(int) + label_pred[mask],\n            minlength=4,\n        ).reshape(2, 2)\n        return hist\n\n    def update(self, pred, ref, ori_size=None):\n        \"\"\"\n        Update confusion matrix for binary IoU.\n\n        Args:\n            pred: Predicted masks [batch, H, W] or [batch, 1, H, W]\n            ref: Ground truth masks [batch, H, W], binary (0, 1)\n            ori_size: Original sizes [(H, W), ...] (optional)\n        \"\"\"\n        pred = np.asarray(pred, np.uint8)\n        ref = np.asarray(ref, np.uint8)\n\n        # Validate ground truth\n        unique_ref = np.unique(ref)\n        if not np.all(np.isin(unique_ref, [0, 1])):\n            raise ValueError(f\"Ground truth masks must be binary (0, 1), got values: {unique_ref}\")\n\n        pred = (pred > 0).astype(np.uint8)\n\n        for i, (r, p) in enumerate(zip(ref, pred)):\n            if ori_size is not None:\n                ori_H, ori_W = ori_size[i]\n                p = p[:ori_H, :ori_W]\n                r = r[:ori_H, :ori_W]\n            self.confusion_matrix += self._fast_hist(r.flatten(), p.flatten())\n\n    def get_scores(self, binary=False, withbg=True):\n        \"\"\"\n        Compute binary IoU from confusion matrix.\n\n        Args:\n            binary: Ignored, always True\n            withbg: If True, return [bg_iou, fg_iou]; else, [fg_iou]\n\n        Returns:\n            iou: np.ndarray, IoU values\n            mean_iou: float, Mean IoU\n        \"\"\"\n        hist = self.confusion_matrix\n        iou = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist) + 1e-7)\n        if not withbg:\n            iou = iou[1:]\n        mean_iou = np.nanmean(iou)\n        return iou, mean_iou\n\n    def reset(self):\n        self.confusion_matrix = np.zeros((2, 2))\n\n\nclass Accumulator(object):\n    def __init__(self, **kwargs):\n        self.values = kwargs\n        self.counter = {k: 0 for k, v in kwargs.items()}\n        for k, v in self.values.items():\n            if not isinstance(v, (float, int, list)):\n                raise TypeError(f\"The Accumulator does not support `{type(v)}`. Supported types: [float, int, list]\")\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(self.values[k], list):\n                self.values[k].append(v)\n            else:\n                self.values[k] = self.values[k] + v\n            self.counter[k] += 1\n\n    def reset(self):\n        for k in self.values.keys():\n            if isinstance(self.values[k], list):\n                self.values[k] = []\n            else:\n                self.values[k] = 0\n            self.counter[k] = 0\n\n    def mean(self, key, axis=None, dic=False):\n        if isinstance(key, str):\n            if isinstance(self.values[key], list):\n                return np.array(self.values[key]).mean(axis)\n            else:\n                return self.values[key] / (self.counter[key] + 1e-7)\n        elif isinstance(key, (list, tuple)):\n            if dic:\n                return {k: self.mean(k, axis) for k in key}\n            return [self.mean(k, axis) for k in key]\n        else:\n            TypeError(f\"`key` must be a str/list/tuple, got {type(key)}\")\n\n    def std(self, key, axis=None, dic=False):\n        if isinstance(key, str):\n            if isinstance(self.values[key], list):\n                return np.array(self.values[key]).std(axis)\n            else:\n                raise RuntimeError(\"`std` is not supported for (int, float). Use list instead.\")\n        elif isinstance(key, (list, tuple)):\n            if dic:\n                return {k: self.std(k) for k in key}\n            return [self.std(k) for k in key]\n        else:\n            TypeError(f\"`key` must be a str/list/tuple, got {type(key)}\")\n\n    def get(self):\n        return {k: v.copy() if isinstance(v, list) else v for k, v in self.values.items()}\n    \n    def load(self, data):\n        for k, v in data.items():\n            if k in self.values:\n                self.values[k] = v.copy() if isinstance(v, list) else v\n                self.counter[k] = len(v) if isinstance(v, list) else 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:54.020565Z","iopub.execute_input":"2025-05-06T21:57:54.020822Z","iopub.status.idle":"2025-05-06T21:57:54.039115Z","shell.execute_reply.started":"2025-05-06T21:57:54.020804Z","shell.execute_reply":"2025-05-06T21:57:54.038463Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/core/metrics.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/data_kits/binary_dataset.py\n# %load /kaggle/working/FPTrans/data_kits/binary_dataset.py\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport cv2\nimport os\n\nclass BinarySegmentationDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.image_filenames = sorted(os.listdir(image_dir))\n        self.mask_filenames = sorted(os.listdir(mask_dir))\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n\n        # Load image and mask\n        image = cv2.imread(image_path)  # BGR image\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask in grayscale\n\n        # Convert mask: 255 -> 1 (to match FPTrans format)\n        mask = np.where(mask == 255, 1, 0).astype(np.uint8)\n\n        # Apply transformations if provided\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image, mask = transformed[\"image\"], transformed[\"mask\"]\n\n        # Convert to tensors\n        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n        mask = torch.tensor(mask, dtype=torch.long)  # No need to normalize mask\n\n        return image, mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:54.039885Z","iopub.execute_input":"2025-05-06T21:57:54.040112Z","iopub.status.idle":"2025-05-06T21:57:54.056199Z","shell.execute_reply.started":"2025-05-06T21:57:54.040096Z","shell.execute_reply":"2025-05-06T21:57:54.055476Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/FPTrans/data_kits/binary_dataset.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/data_kits/tcp_few_shot.py\n# %load  /kaggle/working/FPTrans/data_kits/tcp_few_shot.py\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nfrom constants import project_dir, data_dir\nfrom utils_.misc import load_image\n\ncache_image = {}\ncache_label = {}\n\n\nclass TCPFewShot(Dataset):\n    def __init__(self, opt, split, shot, query,\n                 data_root=None, data_list=None, transform=None, mode='train',\n                 cache=True):\n        assert mode in ['train', 'val', 'test', 'eval_online']\n\n        if mode != \"train\" and opt.dataset in [\"TCP\"]:\n            mode = \"val\"\n        self.opt = opt\n        self.mode = mode\n        self.split = split\n        self.shot = shot\n        self.query = query\n        self.data_root = data_root\n        self.transform = transform\n        self.cache = cache\n\n        self.nclass = 2\n        self.data_list = [Path(x) for x in (project_dir / 'data/tcp/tcp.txt').read_text().splitlines()]\n        test_range = slice(*opt.test_range)\n        self.data_list = self.data_list[test_range]\n        self.length_data_list = len(self.data_list)\n        self.tasks = list(range(len(self)))\n        self.support_image_paths = [list(map(Path, x.split())) for x in (project_dir / 'data/tcp/support.txt').read_text().splitlines()]\n\n        self.sampler = None\n        self.reset_sampler()\n\n    def __len__(self):\n        return self.opt.test_n or self.length_data_list\n\n    def reset_sampler(self):\n        seed = self.opt.seed\n        test_seed = self.opt.test_seed\n        # Use fixed test sampler(opt.test_seed) for reproducibility\n        self.sampler = np.random.RandomState(test_seed)\n\n    def sample_tasks(self):\n        pass\n\n    def seg_encode(self, lab, cls, ignore_lab):\n        if self.opt.proc == 'pil':\n            lab = np.array(lab, np.uint8)\n            lab[lab == 255] = 1  # Convert 255 to 1\n        if len(lab.shape) == 3:\n            lab = lab[:, :, -1]\n        assert len(lab.shape) == 2, lab.shape\n        target_pix = np.where(lab == cls)\n        lab[:, :] = 0\n        if target_pix[0].shape[0] > 0:\n            lab[target_pix[0], target_pix[1]] = 1\n        if self.opt.proc == 'pil':\n            lab = Image.fromarray(lab)\n        return lab\n\n    def get_image(self, name, cache=True):\n        if self.cache and cache:\n            if name not in cache_image:\n                cache_image[name] = load_image(data_dir / name, 'img', self.opt.proc)\n            return cache_image[name].copy()\n        else:\n            return load_image(data_dir / name, 'img', self.opt.proc)\n\n    def get_label(self, name, cls, ignore_lab=0, cache=True):\n        if self.cache and cache:\n            if name not in cache_label:\n                cache_label[name] = load_image(data_dir / name, 'lab', self.opt.proc)\n            lab = cache_label[name].copy()\n        else:\n            lab = load_image(data_dir / name, 'lab', self.opt.proc)\n        lab = self.seg_encode(lab, cls, ignore_lab)\n        lab[lab == 255] = 1  # Convert 255 to 1\n        return lab\n\n    def __getitem__(self, index):\n        qry_idx = self.tasks[index]\n        image_path = self.data_list[qry_idx]\n        sup_image_paths = []\n        sup_label_paths = []\n        for x_path, y_path in self.support_image_paths:\n            sup_image_paths.append(x_path)\n            sup_label_paths.append(y_path)\n\n        qry_names = [image_path.stem]\n        sup_names = [x.stem for x in sup_image_paths]\n\n        ori_image = image = self.get_image(image_path)\n        label = np.zeros(image.shape[:2], dtype=np.uint8)\n        ori_image = torch.from_numpy(ori_image)\n\n        kwargs = {}\n        sup_kwargs = [{} for _ in range(self.opt.shot)]\n        sup_images = [self.get_image(x) for x in sup_image_paths]\n        sup_labels = [self.get_label(x, 255) for x in sup_label_paths]\n\n        # raw_label = label.copy()\n        if self.transform is not None:\n            image, label, kwargs = self.transform[1](image, label, **kwargs)\n            for k in range(self.shot):\n                sup_images[k], sup_labels[k], sup_kwargs[k] = self.transform[0](sup_images[k], sup_labels[k],\n                                                                                **sup_kwargs[k])\n\n        sup_images = torch.stack(sup_images, dim=0)\n        sup_labels = torch.stack(sup_labels, dim=0)\n\n        ret_dict = {\n            'sup_rgb': sup_images,  # [S, 3, H, W]\n            'sup_msk': sup_labels,  # [S, H, W]\n            'qry_rgb': image,       # [3, H, W]\n            'qry_ori': ori_image,   # [H_ori, W_ori, 3]\n            'cls': 255,  # [], values in [1, 20] for PASCAL\n            'sup_names': sup_names,\n            'qry_names': qry_names,\n        }\n\n        ret_dict = {k: v for k, v in ret_dict.items() if v is not None}\n        return ret_dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:54.056964Z","iopub.execute_input":"2025-05-06T21:57:54.057147Z","iopub.status.idle":"2025-05-06T21:57:54.072759Z","shell.execute_reply.started":"2025-05-06T21:57:54.057133Z","shell.execute_reply":"2025-05-06T21:57:54.072109Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/data_kits/tcp_few_shot.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/data_kits/transformation.py\n# %load /kaggle/working/FPTrans/data_kits/transformation.py\n\"\"\"\nA transformation package for image segmentation. It is implemented by PIL and\ntorchvision and its usage is the same as torchvision, except for all the\nclasses accept (img, mask) as input and output (img, mask).\n\nNotes:\nOpencv(cv2)-processed images and PIL-processed images may have some differences,\nwhich may lead to incompatibility between the images and pretrained models and\nperformance drop.\n\"\"\"\n\nimport torchvision.transforms as transforms\nimport torchvision.transforms.functional as tf\nimport numpy as np\nfrom PIL import Image, ImageFilter\nimport random\nimport torch\nimport cv2\n\n__all__ = [\n    \"Compose\",\n    \"Resize\", \"ToTensor\", \"Normalize\",\n    \"ColorJitter\", \"RandomCrop\", \"RandomResize\", \"RandomSizedCrop\",\n    \"RandomHorizontallyFlip\", \"RandomVerticallyFlip\", \"RandomRotate\",\n    \"RandomGaussianBlur\"\n]\nRGB_MEAN = np.array([0.485, 0.456, 0.406]) * 255\n\n\nclass Compose(object):\n    def __init__(self, transformations, processer='pil'):\n        self.transforms = transformations\n        self.processer = processer\n\n    def __call__(self, image, label, **kwargs):\n        if self.processer == 'cv2':\n            image = np.float32(image)\n            label[label == 255] = 1  # Convert 255 to 1\n            label = np.uint8(label)\n        for t in self.transforms:\n            image, label, kwargs = t(image, label, proc=self.processer, **kwargs)\n            \n        return image, label, kwargs\n\n\n###############################################################################\n#\n#   Deterministic Algorithms\n#\n###############################################################################\n\nclass Resize(object):\n    \"\"\"\n    Resize a pair of (image, mask).\n    If `height` and `width` are both specified, then the aspect ratio is not\n    keeped. If only one of the `height` and `width` is specified, the aspect\n    ratio will be preserved.\n\n    Parameters\n    ----------\n    height: int\n    width: int\n    do_mask: bool\n        Do resize on mask or not.\n    \"\"\"\n    def __init__(self, height=None, width=None, do_mask=True):\n        if height is None and width is None:\n            raise ValueError('At least specify one of the `height` and '\n                             '`width`')\n        self.height = height\n        self.width = width\n        self.do_mask = do_mask\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        if proc == 'pil':\n            w, h = img.size\n        else:\n            h, w, _ = img.shape\n        ow, oh = self.width, self.height\n        if self.height is None:\n            oh = int(self.width * h / w)\n        if self.width is None:\n            ow = int(self.height * w / h)\n\n        if proc == 'pil':\n            img = img.resize((ow, oh), Image.BILINEAR)\n            if self.do_mask:\n                mask = mask.resize((ow, oh), Image.NEAREST)\n        else:\n            img = cv2.resize(img, (ow, oh), interpolation=cv2.INTER_LINEAR)\n            if self.do_mask:\n                mask = cv2.resize(mask, (ow, oh), interpolation=cv2.INTER_NEAREST)\n\n        if 'weights' in kwargs:\n            kwargs['weights'] = cv2.resize(kwargs['weights'], (ow, oh), interpolation=cv2.INTER_LINEAR)\n\n        return img, mask, kwargs\n\n\nclass ToTensor(object):\n    def __init__(self, mask_dtype):\n        self.mask_dtype = mask_dtype\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        img = torch.from_numpy(\n            np.ascontiguousarray(np.asarray(img).transpose(2, 0, 1))\n        ).float().div(255)\n\n        mask = torch.from_numpy(np.array(mask, np.uint8))\n        mask[mask == 255] = 1  # Convert 255 to 1\n        if self.mask_dtype == 'long':\n            mask = mask.long()\n        elif self.mask_dtype == 'float':\n            mask = mask.float()\n\n        if 'weights' in kwargs:\n            kwargs['weights'] = torch.from_numpy(kwargs['weights']).float()\n\n        return img, mask, kwargs\n\n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        img = tf.normalize(img, self.mean, self.std)\n\n        return img, mask, kwargs\n\n\n###############################################################################\n#\n#   Random Algorithms\n#\n###############################################################################\n\nclass ColorJitter(object):\n    def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.5):\n        self.p = p\n        self.jitter = transforms.ColorJitter(\n            brightness, contrast, saturation, hue\n        )\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        if random.random() > self.p:\n            if proc == 'pil':\n                img = self.jitter(img)\n            else:\n                img = Image.fromarray(img.astype(np.uint8))\n                img = self.jitter(img)\n                img = np.array(img)\n\n        return img, mask, kwargs\n\n\nclass RandomGaussianBlur(object):\n    def __init__(self, radius=5, rand_radius=False):\n        self.radius = radius\n        self.rand_radius = rand_radius\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        if random.random() < 0.5:\n            if proc == 'pil':\n                img = img.filter(ImageFilter.GaussianBlur(self.radius))\n            else:\n                radius = random.choice([self.radius, self.radius - 2, self.radius + 2]) if self.rand_radius else self.radius\n                img = cv2.GaussianBlur(img, (radius, radius), 0)\n\n        return img, mask, kwargs\n\n\nclass RandomCrop(object):\n    \"\"\"\n    Random crop a patch from the (img, mask) pair.\n\n    Parameters\n    ----------\n    height: int\n        Output height\n    width: int\n        Output width\n    check: bool\n        Whether to check the cropped mask contains a minimum proportion of\n        original foreground/background pixels. Only support two classes\n    prop: float\n        Foreground pixel proportion.\n    max_iter: int\n        Maximum iterations to try (to meet the `check` requirement).\n    \"\"\"\n    def __init__(self, height, width, check=False, prop=0.85, max_iter=30, center=True, pad_type='reflect'):\n        self.height = height\n        self.width = width\n        self.check = check\n        self.prop = prop\n        self.max_iter = max_iter\n        self.center = center\n        if pad_type == 'reflect':\n            self.pad_kwargs_image = self.pad_kwargs_mask = self.pad_kwargs_weight = {\n                'borderType': cv2.BORDER_REFLECT101\n            }\n        elif pad_type == 'constant':\n            self.pad_kwargs_image = {\n                'borderType': cv2.BORDER_CONSTANT,\n                'value': RGB_MEAN\n            }\n            self.pad_kwargs_mask = {\n                'borderType': cv2.BORDER_CONSTANT,\n                'value': 255,\n            }\n            self.pad_kwargs_weight = {\n                'borderType': cv2.BORDER_CONSTANT,\n                'value': 0,\n            }\n        else:\n            raise ValueError\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        if proc == 'pil':\n            w, h = img.size\n            mw, mh = mask.size\n        else:\n            h, w, _ = img.shape\n            mh, mw = mask.shape\n        tw, th = self.width, self.height\n        assert w == mw and h == mh, f'(W, H) => ({w}, {h}) vs ({mw}, {mh})'\n        weights = kwargs.get('weights', None)\n\n        if proc not in ['pil', 'cv2']:\n            raise ValueError\n\n        if w != tw or h != th:\n            if w < tw or h < th:\n                if proc == 'pil':\n                    img = img.resize((tw, th), Image.BILINEAR)\n                    mask = mask.resize((tw, th), Image.NEAREST)\n                else:\n                    # img = cv2.resize(img, (tw, th), interpolation=cv2.INTER_LINEAR)\n                    # mask = cv2.resize(mask, (tw, th), interpolation=cv2.INTER_NEAREST)\n                    pad_h = max(th - h, 0)\n                    pad_w = max(tw - w, 0)\n                    if self.center:\n                        pad_h_half = int(pad_h / 2)\n                        pad_w_half = int(pad_w / 2)\n                    else:\n                        pad_h_half = random.randint(0, pad_h)\n                        pad_w_half = random.randint(0, pad_w)\n                    borders = (pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half)\n                    if pad_h > 0 or pad_w > 0:\n                        img = cv2.copyMakeBorder(img, *borders, **self.pad_kwargs_image)\n                        mask = cv2.copyMakeBorder(mask, *borders, **self.pad_kwargs_mask)\n                if weights is not None:\n                    weights = cv2.copyMakeBorder(weights, *borders, **self.pad_kwargs_weight)\n\n            raw_img = img\n            raw_mask = mask\n            raw_weights = weights\n            if proc == 'pil':\n                w, h = img.size\n            else:\n                h, w, _ = img.shape\n            y1 = random.randint(0, h - th)\n            x1 = random.randint(0, w - tw)\n            if proc == 'pil':\n                img = raw_img.crop((x1, y1, x1 + tw, y1 + th))\n                mask = raw_mask.crop((x1, y1, x1 + tw, y1 + th))\n            else:\n                img = raw_img[y1:y1 + th, x1:x1 + tw]\n                mask = raw_mask[y1:y1 + th, x1:x1 + tw]\n\n            if weights is not None:\n                weights = weights[y1:y1 + th, x1:x1 + tw]\n\n            if self.check:\n                # For semgentation, we must assert the cropped patch contains\n                # foreground pixels\n                np_raw_mask = np.array(raw_mask, np.uint8)\n                np_mask = np.array(mask, np.uint8)\n                np_mask[np_mask == 255] = 1  # Convert 255 to 1\n                raw_pos_num = np.sum(np_raw_mask == 1)\n                pos_num = np.sum(np_mask == 1)\n                crop_iter = 0\n                while pos_num < self.prop * raw_pos_num and crop_iter <= self.max_iter:\n                    image = raw_img\n                    label = raw_mask\n                    weights = raw_weights\n                    y1 = random.randint(0, h - th)\n                    x1 = random.randint(0, w - tw)\n                    if proc == 'pil':\n                        img = raw_img.crop((x1, y1, x1 + tw, y1 + th))\n                        mask = raw_mask.crop((x1, y1, x1 + tw, y1 + th))\n                    else:\n                        img = raw_img[y1:y1 + th, x1:x1 + tw]\n                        mask = raw_mask[y1:y1 + th, x1:x1 + tw]\n                    if weights is not None:\n                        weights = weights[y1:y1 + th, x1:x1 + tw]\n                    crop_iter += 1\n                if crop_iter >= self.max_iter:\n                    if proc == 'pil':\n                        img = raw_img.resize((tw, th), Image.BILINEAR)\n                        mask = raw_mask.resize((tw, th), Image.NEAREST)\n                    else:\n                        img = cv2.resize(img, (tw, th), interpolation=cv2.INTER_LINEAR)\n                        mask = cv2.resize(mask, (tw, th), interpolation=cv2.INTER_NEAREST)\n                    if weights is not None:\n                        weights = cv2.resize(weights, (tw, th), interpolation=cv2.INTER_LINEAR)\n\n        if weights is not None:\n            kwargs['weights'] = weights\n\n        return img, mask, kwargs\n\n\nclass RandomResize(object):\n    def __init__(self, smin, smax):\n        self.smin = smin\n        self.smax = smax\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        if proc == 'pil':\n            w, h = img.size\n            mw, mh = mask.size\n        else:\n            h, w, _ = img.shape\n            mh, mw = mask.shape\n        assert w == mw and h == mh, f'(W, H) => ({w}, {h}) vs ({mw}, {mh})'\n\n        scale = self.smin + (self.smax - self.smin) * random.random()\n\n        tw = int(w * scale)\n        th = int(h * scale)\n\n        if proc == 'pil':\n            img = img.resize((tw, th), Image.BILINEAR)\n            mask = mask.resize((tw, th), Image.NEAREST)\n        else:\n            img = cv2.resize(img, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n            mask = cv2.resize(mask, None, fx=scale, fy=scale, interpolation=cv2.INTER_NEAREST)\n\n        if 'weights' in kwargs:\n            kwargs['weights'] = cv2.resize(kwargs['weights'], None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n\n        return img, mask, kwargs\n\n\nclass RandomSizedCrop(object):\n    def __init__(self, smin, smax, height, width,\n                 check=False, fg_prop=0.85, bg_prop=0.15, max_iter=30, center=True, pad_type='reflect'):\n        self.resize = RandomResize(smin, smax)\n        self.crop = RandomCrop(height, width, check, fg_prop, bg_prop, max_iter, center, pad_type)\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        img, mask, kwargs = self.resize(img, mask, proc=proc, **kwargs)\n        img, mask, kwargs = self.crop(img, mask, proc=proc, **kwargs)\n        return img, mask, kwargs\n\n\nclass RandomHorizontallyFlip(object):\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        if random.random() < self.p:\n            if proc == 'pil':\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n                mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n            else:\n                img = np.ascontiguousarray(np.flip(img, axis=1))\n                mask = np.ascontiguousarray(np.flip(mask, axis=1))\n            if 'weights' in kwargs:\n                kwargs['weights'] = np.ascontiguousarray(np.flip(kwargs['weights'], axis=1))\n\n        return img, mask, kwargs\n\n\nclass RandomVerticallyFlip(object):\n    def __init__(self, p):\n        self.p = p\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        if random.random() < self.p:\n            if proc == 'pil':\n                img = img.transpose(Image.FLIP_TOP_BOTTOM)\n                mask = mask.transpose(Image.FLIP_TOP_BOTTOM)\n            else:\n                img = np.ascontiguousarray(np.flip(img, axis=0))\n                mask = np.ascontiguousarray(np.flip(mask, axis=0))\n            if 'weights' in kwargs:\n                kwargs['weights'] = np.ascontiguousarray(np.flip(kwargs['weights'], axis=0))\n\n        return img, mask, kwargs\n\n\nclass RandomRotate(object):\n    def __init__(self, rotate, p=0.5, pad_type='reflect'):\n        self.rotate = rotate\n        self.p = p\n        if pad_type == 'reflect':\n            self.pad_kwargs_image = self.pad_kwargs_mask = self.pad_kwargs_weight = {\n                'borderMode': cv2.BORDER_REFLECT101\n            }\n        elif pad_type == 'constant':\n            self.pad_kwargs_image = {\n                'borderMode': cv2.BORDER_CONSTANT,\n                'borderValue': RGB_MEAN,\n            }\n            self.pad_kwargs_mask = {\n                'borderMode': cv2.BORDER_CONSTANT,\n                'borderValue': 255,\n            }\n            self.pad_kwargs_weight = {\n                'borderMode': cv2.BORDER_CONSTANT,\n                'borderValue': 0,\n            }\n        else:\n            raise ValueError\n\n    def __call__(self, img, mask, proc='pil', **kwargs):\n        if random.random() < self.p:\n            angle = -self.rotate + 2 * self.rotate * random.random()\n\n            if proc == 'pil':\n                img, mask = np.array(img, np.uint8), np.array(mask, np.uint8)\n\n            h, w, _ = img.shape\n            matrix = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1)\n            img = cv2.warpAffine(img, matrix, (w, h), flags=cv2.INTER_LINEAR, **self.pad_kwargs_image)\n            mask = cv2.warpAffine(mask, matrix, (w, h), flags=cv2.INTER_NEAREST, **self.pad_kwargs_mask)\n            if proc == 'pil':\n                img, mask = Image.fromarray(img), Image.fromarray(mask)\n\n            if 'weights' in kwargs:\n                kwargs['weights'] = cv2.warpAffine(kwargs['weights'], matrix, (w, h), flags=cv2.INTER_LINEAR, **self.pad_kwargs_weight)\n        \n        return img, mask, kwargs\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:54.088101Z","iopub.execute_input":"2025-05-06T21:57:54.088677Z","iopub.status.idle":"2025-05-06T21:57:54.100073Z","shell.execute_reply.started":"2025-05-06T21:57:54.088656Z","shell.execute_reply":"2025-05-06T21:57:54.099484Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/data_kits/transformation.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/run.py\n# %load /kaggle/working/FPTrans/run.py\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom sacred import Experiment\n\nfrom config import setup, init_environment\nfrom constants import on_cloud\nfrom core.base_trainer import BaseTrainer, BaseEvaluator\nfrom core.losses import get as get_loss_obj\nfrom data_kits import datasets\nfrom networks import load_model\nfrom utils_ import misc\n\nex = setup(\n    Experiment(name=\"FPTrans\", save_git_info=False, base_dir=\"./\")\n)\ntorch.set_printoptions(precision=8)\n\nclass Evaluator(BaseEvaluator):\n    def test_step(self, batch, step):\n        sup_rgb = batch['sup_rgb'].cuda()\n        sup_msk = batch['sup_msk'].cuda()\n        qry_rgb = batch['qry_rgb'].cuda()\n        qry_msk = batch['qry_msk'].cuda()\n        classes = batch['cls'].cuda()\n\n        output = self.model_DP(qry_rgb, sup_rgb, sup_msk, qry_msk)\n        qry_pred = output['out']\n\n        # Compute loss\n        loss = self.loss_obj(qry_pred, qry_msk.squeeze(1))\n\n        # Compute prediction\n        qry_pred = qry_pred.argmax(dim=1).detach().cpu().numpy()\n        return qry_pred, {'loss': loss.item()}\n\n\nclass Trainer(BaseTrainer):\n    def _train_step(self, batch, step, epoch):\n        sup_rgb = batch['sup_rgb'].cuda()\n        sup_msk = batch['sup_msk'].cuda()\n        qry_rgb = batch['qry_rgb'].cuda()\n        qry_msk = batch['qry_msk'].cuda()\n        classes = batch['cls'].cuda()\n        kwargs = {}\n        if 'weights' in batch:\n            kwargs['weight'] = batch['weights'].cuda()\n\n        output = self.model_DP(qry_rgb, sup_rgb, sup_msk, qry_msk)\n        qry_msk_reshape = qry_msk.view(-1, *qry_msk.shape[-2:])\n\n        loss = self.loss_obj(output['out'], qry_msk_reshape, **kwargs)        \n        loss_prompt = self.loss_obj(output['out_prompt'], qry_msk_reshape, **kwargs)\n        if len(output['loss_pair'].shape) == 0:     # single GPU\n            loss_pair = output['loss_pair']\n        else:   # multiple GPUs\n            loss_pair = output['loss_pair'].mean(0)\n        loss_pair = loss_pair * self.opt.pair_lossW\n \n        total_loss = loss + loss_prompt + loss_pair\n        return total_loss, loss, loss_prompt, loss_pair\n\n    def train_step(self, batch, step, epoch):\n        self.optimizer.zero_grad()\n\n        total_loss, loss, loss_prompt, loss_pair = self._train_step(batch, step, epoch)\n\n        total_loss.backward()\n        self.optimizer.step()\n        \n        return {\n            'loss': loss.item(),\n            'prompt': loss_prompt.item(),\n            'pair': loss_pair.item(),\n        }\n\n\n@ex.main\ndef train(_run, _config):\n    opt, logger, device = init_environment(ex, _run, _config)\n\n    ds_train, data_loader, _ = datasets.load(opt, logger, \"train\")\n    ds_eval_online, data_loader_val, num_classes = datasets.load(opt, logger, \"eval_online\")\n    \n    logger.info(f'     ==> {len(ds_train)} training samples')\n    logger.info(f'     ==> {len(ds_eval_online)} eval_online samples')\n\n    model = load_model(opt, logger)\n    if opt.exp_id >= 0 or opt.ckpt:\n        \n        ckpt = misc.find_snapshot(_run.run_dir.parent, opt.exp_id, opt.ckpt, afs=on_cloud)\n        model.load_weights(ckpt, logger, strict=opt.strict)\n\n    trainer = Trainer(opt, logger, device, model, data_loader, data_loader_val, _run)\n    evaluator = Evaluator(opt, logger, device, trainer.model_DP, None, \"EVAL_ONLINE\")\n\n    logger.info(\"Start training.\")\n    start_epoch = 1\n    trainer.start_training_loop(start_epoch, evaluator, num_classes)\n\n    logger.info(f\"============ Training finished - id {_run._id} ============\\n\")\n    if _run._id is not None:\n        return test(_run, _config, _run._id, ckpt=None, strict=False, eval_after_train=True)\n\n\n@ex.command(unobserved=True)\ndef test(_run, _config, exp_id=-1, ckpt=None, strict=True, eval_after_train=False):\n    opt, logger, device = init_environment(ex, _run, _config, eval_after_train=eval_after_train)\n\n    ds_test, data_loader, num_classes = datasets.load(opt, logger, \"test\")\n    \n    logger.info(f'     ==> {len(ds_test)} testing samples')\n\n    model = load_model(opt, logger)\n    if not opt.no_resume:\n        model_ckpt = misc.find_snapshot(_run.run_dir.parent, exp_id, ckpt)\n        logger.info(f\"     ==> Try to load checkpoint from {model_ckpt}\")\n        model.load_weights(model_ckpt, logger, strict=strict)\n        logger.info(f\"     ==> Checkpoint loaded.\")\n\n    tester = Evaluator(opt, logger, device, model, None, \"EVAL\")\n\n    logger.info(\"Start testing.\")\n    loss, mean_iou, binary_iou, _, _ , catch_rate , yeild_rate = tester.start_eval_loop(data_loader, num_classes)\n\n    return f\"Loss: {loss:.4f}, mIoU: {mean_iou * 100:.2f}, bIoU: {binary_iou * 100:.2f} , catch_rate:{catch_rate * 100:.2f}, yeild_rate: {yeild_rate* 100:.2f}\"\n\n\n@ex.command(unobserved=True)\ndef predict(_run, _config, exp_id=-1, ckpt=None, strict=True):\n    opt, logger, device = init_environment(ex, _run, _config)\n\n    model = load_model(opt, logger)\n    if not opt.no_resume:\n        model_ckpt = misc.find_snapshot(_run.run_dir.parent, exp_id, ckpt)\n        logger.info(f\"     ==> Try to load checkpoint from {model_ckpt}\")\n        model.load_weights(model_ckpt, logger, strict)\n        logger.info(f\"     ==> Checkpoint loaded.\")\n    model = model.to(device)\n    loss_obj = get_loss_obj(opt, logger, loss='ce')\n\n    sup_rgb, sup_msk, qry_rgb, qry_msk, qry_ori = datasets.load_p(opt, device)\n    classes = torch.LongTensor([opt.p.cls]).cuda()\n\n    logger.info(\"Start predicting.\")\n\n    model.eval()\n    ret_values = []\n    for i in range(qry_rgb.shape[0]):\n        print('Processing:', i + 1)\n        qry_rgb_i = qry_rgb[i:i + 1]\n        qry_msk_i = qry_msk[i:i + 1] if qry_msk is not None else None\n        qry_ori_i = qry_ori[i]\n        \n        output = model(qry_rgb_i, sup_rgb, sup_msk, out_shape=qry_ori_i.shape[-3:-1])\n        pred = output['out'].argmax(dim=1).detach().cpu().numpy()\n\n        if qry_msk_i is not None:\n            loss = loss_obj(output['out'], qry_msk_i).item()\n            ref = qry_msk_i.cpu().numpy()\n            tp = int((np.logical_and(pred == 1, ref != 255) * np.logical_and(ref == 1, ref != 255)).sum())\n            fp = int((np.logical_and(pred == 1, ref != 255) * np.logical_and(ref != 1, ref != 255)).sum())\n            fn = int((np.logical_and(pred != 1, ref != 255) * np.logical_and(ref == 1, ref != 255)).sum())\n            mean_iou = tp / (tp + fp + fn)\n            binary_iou = 0\n            ret_values.append(f\"Loss: {loss:.4f}, mIoU: {mean_iou * 100:.2f}, bIoU: {binary_iou * 100:.2f}\")\n        else:\n            ret_values.append(None)\n\n        # Save to file\n        if opt.p.out:\n            pred = pred[0].astype(np.uint8) * 255\n            if opt.p.overlap:\n                out = qry_ori_i.copy()\n                # out[pred == 255] = out[pred == 255] * 0.5 + np.array([255, 0, 0]) * 0.5\n                pred[pred == 255] = 1  # Ensure consistency\n                out[pred == 1] = out[pred == 1] * 0.5 + np.array([255, 0, 0]) * 0.5\n            else:\n                out = pred\n\n            out_dir = Path(opt.p.out)\n            out_dir.mkdir(parents=True, exist_ok=True)\n            out_name = Path(opt.p.qry or opt.p.qry_rgb[i]).stem + '_pred.png'\n            out_path = out_dir / out_name\n            Image.fromarray(out).save(out_path)\n\n        # Release memory\n        del output\n        torch.cuda.empty_cache()\n\n    if ret_values[0] is not None:\n        return '\\n'.join(ret_values)\n\n\nif __name__ == '__main__':\n    ex.run_commandline()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:54.101259Z","iopub.execute_input":"2025-05-06T21:57:54.101459Z","iopub.status.idle":"2025-05-06T21:57:54.121778Z","shell.execute_reply.started":"2025-05-06T21:57:54.101443Z","shell.execute_reply":"2025-05-06T21:57:54.121111Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/run.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%%writefile /kaggle/working/FPTrans/networks/vit.py\n# %load /kaggle/working/FPTrans/networks/vit.py\n\"\"\"Vision Transformer (ViT) in PyTorch\nA PyTorch implement of Vision Transformers as described in:\n'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale'\n    - https://arxiv.org/abs/2010.11929\n`How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers`\n    - https://arxiv.org/abs/2106.10270\nThe official jax code is released and available at https://github.com/google-research/vision_transformer\nDeiT model defs and weights from https://github.com/facebookresearch/deit,\npaper `DeiT: Data-efficient Image Transformers` - https://arxiv.org/abs/2012.12877\nAcknowledgments:\n* The paper authors for releasing code and weights, thanks!\n* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\nfor some einops/einsum fun\n* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\nHacked together by / Copyright 2021 Ross Wightman\n\n# ===========================================================================================\n# Feature-Proxy Transformer (FPTrans) in PyTorch\n#\n# This code file is copied and modified from\n#     https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py\n# for the development of FPTrans.\n#\n# By Jian-Wei Zhang (zjw.math@qq.com).\n# 2022-09\n#\n# ===========================================================================================\n\n\"\"\"\nimport math\nimport logging\nfrom functools import partial\nfrom collections import OrderedDict\nfrom pathlib import Path\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom networks import vit_utils as utils\n\n_logger = logging.getLogger(name=Path(__file__).parents[1].stem)\n\nIMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\nIMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\nIMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic', 'fixed_input_size': True,\n        'mean': IMAGENET_INCEPTION_MEAN, 'std': IMAGENET_INCEPTION_STD,\n        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    # patch models (weights from official Google JAX impl)\n    'vit_tiny_patch16_224': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz'),\n    'vit_tiny_patch16_384': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_small_patch32_224': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz'),\n    'vit_small_patch32_384': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_small_patch16_224': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz'),\n    'vit_small_patch16_384': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_base_patch32_224': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_224.npz'),\n    'vit_base_patch32_384': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_base_patch16_224': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz'),\n    'vit_base_patch16_384': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_base_patch8_224': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_8-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_224.npz'),\n    'vit_large_patch32_224': _cfg(\n        url='',  # no official model weights for this combo, only for in21k\n    ),\n    'vit_large_patch32_384': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n        input_size=(3, 384, 384), crop_pct=1.0),\n    'vit_large_patch16_224': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_224.npz'),\n    'vit_large_patch16_384': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_384.npz',\n        input_size=(3, 384, 384), crop_pct=1.0),\n\n    # patch models, imagenet21k (weights from official Google JAX impl)\n    'vit_tiny_patch16_224_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0.npz',\n        num_classes=21843),\n    'vit_small_patch32_224_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz',\n        num_classes=21843),\n    'vit_small_patch16_224_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz',\n        num_classes=21843),\n    'vit_base_patch32_224_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.03-do_0.0-sd_0.0.npz',\n        num_classes=21843),\n    'vit_base_patch16_224_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz',\n        num_classes=21843),\n    'vit_base_patch8_224_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/B_8-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz',\n        num_classes=21843),\n    'vit_large_patch32_224_in21k': _cfg(\n        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch32_224_in21k-9046d2e7.pth',\n        num_classes=21843),\n    'vit_large_patch16_224_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1.npz',\n        num_classes=21843),\n    'vit_huge_patch14_224_in21k': _cfg(\n        url='https://storage.googleapis.com/vit_models/imagenet21k/ViT-H_14.npz',\n        hf_hub='timm/vit_huge_patch14_224_in21k',\n        num_classes=21843),\n\n    # SAM trained models (https://arxiv.org/abs/2106.01548)\n    'vit_base_patch32_sam_224': _cfg(\n        url='https://storage.googleapis.com/vit_models/sam/ViT-B_32.npz'),\n    'vit_base_patch16_sam_224': _cfg(\n        url='https://storage.googleapis.com/vit_models/sam/ViT-B_16.npz'),\n\n    # deit models (FB weights)\n    'deit_tiny_patch16_224': _cfg(\n        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'deit_small_patch16_224': _cfg(\n        url='https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'deit_base_patch16_224': _cfg(\n        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n    'deit_base_patch16_384': _cfg(\n        url='https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, input_size=(3, 384, 384), crop_pct=1.0),\n    'deit_tiny_distilled_patch16_224': _cfg(\n        url='https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, classifier=('head', 'head_dist')),\n    'deit_small_distilled_patch16_224': _cfg(\n        url='https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, classifier=('head', 'head_dist')),\n    'deit_base_distilled_patch16_224': _cfg(\n        url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, classifier=('head', 'head_dist')),\n    'deit_base_distilled_patch16_384': _cfg(\n        url='https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth',\n        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD, input_size=(3, 384, 384), crop_pct=1.0,\n        classifier=('head', 'head_dist')),\n\n    # ViT ImageNet-21K-P pretraining by MILL\n    'vit_base_patch16_224_miil_in21k': _cfg(\n        url='https://miil-public-eu.oss-eu-central-1.aliyuncs.com/model-zoo/ImageNet_21K_P/models/timm/vit_base_patch16_224_in21k_miil.pth',\n        mean=(0, 0, 0), std=(1, 1, 1), crop_pct=0.875, interpolation='bilinear', num_classes=11221,\n    ),\n    'vit_base_patch16_224_miil': _cfg(\n        url='https://miil-public-eu.oss-eu-central-1.aliyuncs.com/model-zoo/ImageNet_21K_P/models/timm'\n            '/vit_base_patch16_224_1k_miil_84_4.pth',\n        mean=(0, 0, 0), std=(1, 1, 1), crop_pct=0.875, interpolation='bilinear',\n    ),\n}\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = utils.DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = utils.Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer\n    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n        - https://arxiv.org/abs/2010.11929\n    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`\n        - https://arxiv.org/abs/2012.12877\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 pretrained=\"\",\n                 num_heads=12, mlp_ratio=4., qkv_bias=True, representation_size=None, distilled=False,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., embed_layer=utils.PatchEmbed, norm_layer=None,\n                 act_layer=None, weight_init='',\n                 opt=None, logger=None, original=False):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int, tuple): patch size\n            in_chans (int): number of input channels\n            num_classes (int): number of classes for classification head\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n            distilled (bool): model includes a distillation token and head as in DeiT models\n            drop_rate (float): dropout rate\n            attn_drop_rate (float): attention dropout rate\n            drop_path_rate (float): stochastic depth rate\n            embed_layer (nn.Module): patch embedding layer\n            norm_layer: (nn.Module): normalization layer\n            weight_init: (str): weight init scheme\n        \"\"\"\n        super().__init__()\n        self.opt = opt\n        self.logger = logger\n        self.allow_mod = not original\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.num_tokens = 2 if distilled else 1\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        act_layer = act_layer or nn.GELU\n\n        # Patch embedding\n        self.patch_embed = embed_layer(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, stride=opt.vit_stride)\n        num_patches = self.patch_embed.num_patches\n\n        # prompt tokens\n        self.prompt_tokens = None\n        if self.allow_mod:\n            ncls = 15 if opt.dataset == \"PASCAL\" else 60\n            if opt.coco2pascal:\n                ncls = 60\n            divider = 1 + opt.bg_num * opt.shot\n            self.prompt_tokens = nn.Parameter(\n                torch.zeros(ncls * divider, opt.num_prompt // divider, embed_dim))  # [bank_size, G, embed_dim]\n            nn.init.normal_(self.prompt_tokens.permute(2, 0, 1), std=opt.pt_std)\n            self.sampler = np.random.RandomState(1234)\n\n        # Class token, Distillation token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.blocks = nn.Sequential(*[\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, drop=drop_rate,\n                attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, act_layer=act_layer)\n            for i in range(depth)])\n        self.norm = norm_layer(embed_dim)\n\n        # Representation layer\n        if representation_size and not distilled:\n            self.num_features = representation_size\n            self.pre_logits = nn.Sequential(OrderedDict([\n                ('fc', nn.Linear(embed_dim, representation_size)),\n                ('act', nn.Tanh())\n            ]))\n        else:\n            self.pre_logits = nn.Identity()\n\n        # Classifier head(s)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n        self.head_dist = None\n        if distilled:\n            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n        if pretrained == \"\":\n            self.init_weights(weight_init)\n        else:\n            if str(pretrained).endswith('.pth'):\n                _load_weights_pth(logger, self, pretrained)\n            elif str(pretrained).endswith('.npz'):\n                _load_weights_npz(self, pretrained)\n            else:\n                raise ValueError(f'Not recognized file {pretrained}. [.pth|.npz]')\n\n            if logger is not None:\n                logger.info(' ' * 5 + f'==> {opt.backbone} initialized from {pretrained}')\n\n    def init_weights(self, mode=''):\n        assert mode in ('jax', 'jax_nlhb', 'nlhb', '')\n        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.\n        utils.trunc_normal_(self.pos_embed, std=.02)\n        if self.dist_token is not None:\n            utils.trunc_normal_(self.dist_token, std=.02)\n        if mode.startswith('jax'):\n            # leave cls token as zeros to match jax impl\n            utils.named_apply(partial(_init_vit_weights, head_bias=head_bias, jax_impl=True), self)\n        else:\n            utils.trunc_normal_(self.cls_token, std=.02)\n            self.apply(_init_vit_weights)\n\n    def forward_original(self, x):\n        # x: [B, C, H, W]\n        x = self.patch_embed(x)  # [B, N ,C]\n        cls_token = expand_to_batch(self.cls_token, x.shape[0])  # [B, 1, C]\n        if self.dist_token is None:\n            x = cat_token(cls_token, x)     # [B, N+1, C]\n        else:\n            dist_token = expand_to_batch(self.dist_token, x.shape[0])  # [B, 1, C]\n            x = cat_token(cls_token, dist_token, x)     # [B, N+2, C]\n        x = x + self.pos_embed  # [B, N + 1, C]\n        x = self.pos_drop(x)\n\n        for i, block in enumerate(self.blocks):\n            x = block(x)\n        x = self.norm(x)\n\n        x = x[:, self.num_tokens:, :]\n        b, n, c = x.shape\n        hh = int(math.sqrt(n))\n        x = x.view(b, hh, hh, c).permute(0, 3, 1, 2).contiguous()\n\n        return dict(out=x)\n\n    def forward(self, x):\n        if not self.allow_mod:\n            return self.forward_original(x)\n\n        # x: [B, C, H, W]\n        # fg_token: [B, 1, C]\n        # bg_token: [B, k, C]\n        x, (fg_token, bg_token) = x\n        bank_size, G, embed_dim = self.prompt_tokens.shape\n        B = x.shape[0] // (1 + self.opt.shot)\n        divider = 1 + self.opt.bg_num * self.opt.shot\n        prompts = self.prompt_tokens[self.sampler.choice(bank_size, size=B * divider, replace=False)] \\\n            .reshape(B, divider * G, embed_dim)    # [B, (1+bg_num*shot)*G, embed_dim]\n\n        # print(\"bg_token shape:\", bg_token.shape)\n        # print(\"prompts shape:\", prompts.shape)\n        # print(\"G:\", G)\n        # print(\"reshaping to:\", bg_token.unsqueeze(2).expand(-1, -1, G, -1).shape)\n        value = \"\"\n        if(self.opt.dataset == \"VISION24\"):\n            bg_token = bg_token[:, 0:1, :]  # shape: [1, 1, 768]\n            bg = bg_token.unsqueeze(2)                    # [1, 1, 1, 768]\n            bg = bg.expand(B, (divider - 1), G, embed_dim)  # [3, 1, 12, 768]\n            bg = bg.reshape(B, (divider - 1) * G, embed_dim)  # [3, 12, 768]\n            value = prompts[:, G:] + bg\n\n        else:       \n            value = prompts[:, G:] + bg_token.unsqueeze(2).expand(-1, -1, G, -1).reshape(\n                    B, (divider - 1) * G, embed_dim)\n        tokens = {\n            'fg': prompts[:, :G] + fg_token,\n            \n            'bg': value\n        }\n        x = self.patch_embed(x)  # [B, N ,C]\n        cls_token = expand_to_batch(self.cls_token, x.shape[0])  # [B, 1, C]\n        if self.dist_token is None:\n            x = cat_token(cls_token, x)     # [B, N+1, C]\n        else:\n            dist_token = expand_to_batch(self.dist_token, x.shape[0])  # [B, 1, C]\n            x = cat_token(cls_token, dist_token, x)     # [B, N+2, C]\n        x = x + self.pos_embed  # [B, N + 1, C']\n        x = self.pos_drop(x)\n\n        S = self.opt.shot\n        B = x.shape[0] // (S + 1)\n\n        # Concat image and tokens\n        _, N, C = x.shape\n        n1 = tokens['fg'].shape[1]\n        n2 = tokens['bg'].shape[1]\n        fg_token = tokens['fg'].view(B, 1, n1, C).expand(-1, S + 1, -1, -1).reshape(B*(S+1), n1, C)\n        bg_token = tokens['bg'].view(B, 1, n2, C).expand(-1, S + 1, -1, -1).reshape(B*(S+1), n2, C)\n        x = cat_token(x, fg_token, bg_token)\n\n        # Forward transformer blocks\n        for i, block in enumerate(self.blocks):\n            x = block(x)\n            x = self.reduce_and_expand(x, num=S + 1, start=self.num_tokens, end=n1 + n2)\n\n        # Split outputs\n        x, fg_token, bg_token = x.split([N, n1, n2], dim=1)\n        tokens['fg'] = fg_token.view(B, S + 1, n1, C)[:, 0]\n        tokens['bg'] = bg_token.view(B, S + 1, n2, C)[:, 0]\n\n        x = self.norm(x)\n\n        x = x[:, self.num_tokens:, :]\n        b, n, c = x.shape\n        hh = int(math.sqrt(n))\n        x = x.view(b, hh, hh, c).permute(0, 3, 1, 2).contiguous()\n\n        tokens['fg'] = tokens['fg'].mean(1)[:, :, None, None]    # [B, C, 1, 1]\n        if self.opt.bg_num == 1:\n            tokens['bg'] = tokens['bg'].mean(1)[:, :, None, None]    # [B, C, 1, 1]\n        else:\n            _B, _N, _C = tokens['bg'].shape\n            bg_num = self.opt.bg_num\n            tokens['bg'] = tokens['bg'].reshape(\n                _B * bg_num, _N // bg_num, _C).mean(1)[:, :, None, None]    # [Bk, C, 1, 1]\n\n        return dict(out=x, tokens=tokens)\n\n    @staticmethod\n    def reduce_and_expand(x, num, start=1, end=0):\n        B, N, C = x.shape\n        x = x.view(B // num, num, N, C)\n        if start > 0:\n            mean_token = x[:, :, :start, :].mean(1, keepdim=True).expand(-1, num, -1, -1)\n            x[:, :, :start, :] = mean_token\n        if end > 0:\n            mean_token = x[:, :, -end:, :].mean(1, keepdim=True).expand(-1, num, -1, -1)\n            x[:, :, -end:, :] = mean_token\n        return x.view(B, N, C)\n\n\ndef expand_to_batch(prompt, batch_size, stack=False):\n    if stack:\n        prompt = prompt.unsqueeze(0)\n    return prompt.expand(batch_size, *[-1 for _ in prompt.shape[1:]])\n\ndef cat_token(*args):\n    return torch.cat(args, dim=1)\n\ndef _init_vit_weights(module: nn.Module, name: str = '', head_bias: float = 0., jax_impl: bool = False):\n    \"\"\" ViT weight initialization\n    * When called without n, head_bias, jax_impl args it will behave exactly the same\n      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).\n    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl\n    \"\"\"\n    if isinstance(module, nn.Linear):\n        if name.startswith('head'):\n            nn.init.zeros_(module.weight)\n            nn.init.constant_(module.bias, head_bias)\n        elif name.startswith('pre_logits'):\n            utils.lecun_normal_(module.weight)\n            nn.init.zeros_(module.bias)\n        else:\n            if jax_impl:\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    if 'mlp' in name:\n                        nn.init.normal_(module.bias, std=1e-6)\n                    else:\n                        nn.init.zeros_(module.bias)\n            else:\n                utils.trunc_normal_(module.weight, std=.02)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n    elif jax_impl and isinstance(module, nn.Conv2d):\n        # NOTE conv was left to pytorch default in my original init\n        utils.lecun_normal_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):\n        nn.init.zeros_(module.bias)\n        nn.init.ones_(module.weight)\n\n\n@torch.no_grad()\ndef _load_weights_npz(model: VisionTransformer, checkpoint_path: str, prefix: str = ''):\n    \"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n    \"\"\"\n    import numpy as np\n\n    def _n2p(w, t=True):\n        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n            w = w.flatten()\n        if t:\n            if w.ndim == 4:\n                w = w.transpose([3, 2, 0, 1])\n            elif w.ndim == 3:\n                w = w.transpose([2, 0, 1])\n            elif w.ndim == 2:\n                w = w.transpose([1, 0])\n        return torch.from_numpy(w)\n\n    w = np.load(checkpoint_path)\n    if not prefix and 'opt/target/embedding/kernel' in w:\n        prefix = 'opt/target/'\n\n    if hasattr(model.patch_embed, 'backbone'):\n        # hybrid\n        backbone = model.patch_embed.backbone\n        stem_only = not hasattr(backbone, 'stem')\n        stem = backbone if stem_only else backbone.stem\n        stem.conv.weight.copy_(utils.adapt_input_conv(stem.conv.weight.shape[1], _n2p(w[f'{prefix}conv_root/kernel'])))\n        stem.norm.weight.copy_(_n2p(w[f'{prefix}gn_root/scale']))\n        stem.norm.bias.copy_(_n2p(w[f'{prefix}gn_root/bias']))\n        if not stem_only:\n            for i, stage in enumerate(backbone.stages):\n                for j, block in enumerate(stage.blocks):\n                    bp = f'{prefix}block{i + 1}/unit{j + 1}/'\n                    for r in range(3):\n                        getattr(block, f'conv{r + 1}').weight.copy_(_n2p(w[f'{bp}conv{r + 1}/kernel']))\n                        getattr(block, f'norm{r + 1}').weight.copy_(_n2p(w[f'{bp}gn{r + 1}/scale']))\n                        getattr(block, f'norm{r + 1}').bias.copy_(_n2p(w[f'{bp}gn{r + 1}/bias']))\n                    if block.downsample is not None:\n                        block.downsample.conv.weight.copy_(_n2p(w[f'{bp}conv_proj/kernel']))\n                        block.downsample.norm.weight.copy_(_n2p(w[f'{bp}gn_proj/scale']))\n                        block.downsample.norm.bias.copy_(_n2p(w[f'{bp}gn_proj/bias']))\n        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n    else:\n        embed_conv_w = utils.adapt_input_conv(\n            model.patch_embed.proj.weight.shape[1], _n2p(w[f'{prefix}embedding/kernel']))\n    model.patch_embed.proj.weight.copy_(embed_conv_w)\n    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n    model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n    pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)\n    if pos_embed_w.shape != model.pos_embed.shape:\n        pos_embed_w = resize_pos_embed(  # resize pos embedding when different size from pretrained weights\n            pos_embed_w, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n    model.pos_embed.copy_(pos_embed_w)\n    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n    if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\n        model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\n        model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\n    if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:\n        model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))\n        model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))\n    for i, block in enumerate(model.blocks.children()):\n        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n        mha_prefix = block_prefix + 'MultiHeadDotProductAttention_1/'\n        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n        block.attn.qkv.weight.copy_(torch.cat([\n            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n        block.attn.qkv.bias.copy_(torch.cat([\n            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n        for r in range(2):\n            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/kernel']))\n            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/bias']))\n        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/scale']))\n        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/bias']))\n\n\n@torch.no_grad()\ndef _load_weights_pth(logger, model: VisionTransformer, checkpoint_path):\n    ckpt = torch.load(checkpoint_path, map_location='cpu')['model']\n    state_dict = model.state_dict()\n    if ckpt['pos_embed'].shape != state_dict['pos_embed'].shape:\n        ckpt['pos_embed'] = resize_pos_embed(\n            ckpt['pos_embed'], state_dict['pos_embed'], model.num_tokens, model.patch_embed.grid_size)\n\n    counter = 0\n    for k in state_dict.keys():\n        if k in ckpt:\n            state_dict[k] = ckpt[k]\n            counter += 1\n\n    logger.info(' ' * 5 + f\"==> {counter} parameters loaded.\")\n    model.load_state_dict(state_dict, strict=True)\n\n\ndef resize_pos_embed(posemb, posemb_new, num_tokens=1, gs_new=()):\n    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n    _logger.info(' ' * 5 + '==> Resized position embedding: %s to %s', posemb.shape, posemb_new.shape)\n    ntok_new = posemb_new.shape[1]\n    if num_tokens:\n        posemb_tok, posemb_grid = posemb[:, :num_tokens], posemb[0, num_tokens:]\n        ntok_new -= num_tokens\n    else:\n        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n    gs_old = int(math.sqrt(len(posemb_grid)))\n    if not len(gs_new):  # backwards compatibility\n        gs_new = [int(math.sqrt(ntok_new))] * 2\n    assert len(gs_new) >= 2\n    _logger.info(' ' * 5 + '==> Position embedding grid-size from %s to %s', [gs_old, gs_old], gs_new)\n    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n    posemb_grid = F.interpolate(posemb_grid, size=gs_new, mode='bicubic', align_corners=False)\n    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new[0] * gs_new[1], -1)\n    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n    return posemb\n\n\ndef checkpoint_filter_fn(state_dict, model):\n    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n    out_dict = {}\n    if 'model' in state_dict:\n        # For deit models\n        state_dict = state_dict['model']\n    for k, v in state_dict.items():\n        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:\n            # For old models that I trained prior to conv based patchification\n            O, I, H, W = model.patch_embed.proj.weight.shape\n            v = v.reshape(O, -1, H, W)\n        elif k == 'pos_embed' and v.shape != model.pos_embed.shape:\n            # To resize pos embedding when using model at different size from pretrained weights\n            v = resize_pos_embed(\n                v, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n        out_dict[k] = v\n    return out_dict\n\n\nvit_factory = {\n    'ViT-Ti/16':         {'patch_size': 16, 'embed_dim':  192, 'depth': 12, 'num_heads':  3, 'distilled': False},\n    'ViT-S/32':          {'patch_size': 32, 'embed_dim':  384, 'depth': 12, 'num_heads':  6, 'distilled': False},\n    'ViT-S/16':          {'patch_size': 16, 'embed_dim':  384, 'depth': 12, 'num_heads':  6, 'distilled': False},\n    'ViT-S/16-i21k':     {'patch_size': 16, 'embed_dim':  384, 'depth': 12, 'num_heads':  6, 'distilled': False},\n    'ViT-B/32':          {'patch_size': 32, 'embed_dim':  768, 'depth': 12, 'num_heads': 12, 'distilled': False},\n    'ViT-B/16':          {'patch_size': 16, 'embed_dim':  768, 'depth': 12, 'num_heads': 12, 'distilled': False},\n    'ViT-B/16-384':      {'patch_size': 16, 'embed_dim':  768, 'depth': 12, 'num_heads': 12, 'distilled': False},\n    'ViT-B/16-i21k':     {'patch_size': 16, 'embed_dim':  768, 'depth': 12, 'num_heads': 12, 'distilled': False},\n    'ViT-B/16-i21k-384': {'patch_size': 16, 'embed_dim':  768, 'depth': 12, 'num_heads': 12, 'distilled': False},\n    'ViT-B/8':           {'patch_size':  8, 'embed_dim':  768, 'depth': 12, 'num_heads': 12, 'distilled': False},\n    'ViT-L/32':          {'patch_size': 32, 'embed_dim': 1024, 'depth': 24, 'num_heads': 16, 'distilled': False},\n    'ViT-L/16':          {'patch_size': 16, 'embed_dim': 1024, 'depth': 24, 'num_heads': 16, 'distilled': False},\n    'ViT-L/16-384':      {'patch_size': 16, 'embed_dim': 1024, 'depth': 24, 'num_heads': 16, 'distilled': False},\n\n    'DeiT-T/16':         {'patch_size': 16, 'embed_dim':  192, 'depth': 12, 'num_heads': 3, 'distilled': True},\n    'DeiT-S/16':         {'patch_size': 16, 'embed_dim':  384, 'depth': 12, 'num_heads': 6, 'distilled': True},\n    'DeiT-B/16':         {'patch_size': 16, 'embed_dim':  768, 'depth': 12, 'num_heads': 12, 'distilled': True},\n    'DeiT-B/16-384':     {'patch_size': 16, 'embed_dim':  768, 'depth': 12, 'num_heads': 12, 'distilled': True},\n}\n\n\ndef vit_model(model_type,\n              image_size,\n              pretrained=\"\",\n              init_channels=3,\n              num_classes=1000,\n              opt=None,\n              logger=None,\n              original=False,\n              depth=None):\n    return VisionTransformer(img_size=image_size,\n                             patch_size=vit_factory[model_type]['patch_size'],\n                             in_chans=init_channels,\n                             num_classes=num_classes,\n                             embed_dim=vit_factory[model_type]['embed_dim'],\n                             depth=depth or opt.vit_depth or vit_factory[model_type]['depth'],\n                             num_heads=vit_factory[model_type]['num_heads'],\n                             pretrained=pretrained,\n                             distilled=vit_factory[model_type]['distilled'],\n                             opt=opt,\n                             logger=logger,\n                             original=original)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T21:57:54.123022Z","iopub.execute_input":"2025-05-06T21:57:54.123208Z","iopub.status.idle":"2025-05-06T21:57:54.154624Z","shell.execute_reply.started":"2025-05-06T21:57:54.123194Z","shell.execute_reply":"2025-05-06T21:57:54.153987Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/FPTrans/networks/vit.py\n","output_type":"stream"}],"execution_count":14}]}